{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNmCZAFpcmdgyctYsWVNYVm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fuentes145/Tarea2/blob/main/parte_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BL-oYKcKICDz"
      },
      "source": [
        "# 2. Finetuning YOLOv5: ``You-Only-Look-Once\""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Actividad 4\n",
        "Investigue sobre la arquitectura de los modelos YOLO y explique los siguientes conceptos (para cada\n",
        "caso use solo 5 l√≠neas m√°ximo):\n",
        "\n",
        "\n",
        "‚Ä¢ Diferencia entre frameworks de detecci√≥n de objetos de una y dos etapas (single-stage vs.\n",
        "two-stage object detection).\n",
        "\n",
        "\n",
        "‚Ä¢ Funci√≥n de p√©rdida Complete Intersection Over Union (CIoU) y su uso en la red YOLOv5.\n",
        "\n",
        "\n",
        "‚Ä¢ Funci√≥n de p√©rdida Binary Cross Entropy (BCE) y su uso en la red YOLOv5.\n",
        "\n",
        "‚Ä¢ ¬øC√≥mo se calcula la funci√≥n de p√©rdida total en YOLOv5?"
      ],
      "metadata": {
        "id": "yG6_FdclFGxg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "## 1.‚ÄØSingle‚Äëstage vs. Two‚Äëstage object detection\n",
        "- **Modelos de una etapa (single‚Äëstage)**, como YOLO, *predicen directamente* bounding boxes‚ÄØ+‚ÄØclases en un **√∫nico paso** sobre la imagen, obteniendo gran velocidad.\n",
        "- **Modelos de dos etapas (two‚Äëstage)**, como Faster‚ÄØR‚ÄëCNN, primero generan **regiones propuestas (RPN)** y luego **refinan y clasifican** cada propuesta.\n",
        "- El enfoque de una etapa sacrifica algo de exactitud por latencia baj√≠sima; el de dos etapas logra mejor mAP pero con mayor coste computacional.\n",
        "- En tareas de *streaming* o tiempo real, la relaci√≥n *precisi√≥n/velocidad* de YOLO lo hace preferible.\n",
        "\n",
        "## 2.‚ÄØP√©rdida Complete¬†IoU (CIoU)\n",
        "La m√©trica CIoU extiende la IoU cl√°sica penalizando tambi√©n la distancia entre centros y la discrepancia de aspecto:\n",
        "\n",
        "$$\n",
        "\\text{CIoU}(b, b^{gt}) = \\text{IoU}\n",
        "- \\frac{\\rho^2(\\mathbf{c}, \\mathbf{c^{gt}})}{c^2}\n",
        "- \\alpha v\n",
        "$$\n",
        "\n",
        "con $\\rho$ la distancia eucl√≠dea entre centros, $c$ la diagonal del cuadro m√≠nimo que los contiene, $v$ la divergencia de relaci√≥n ancho‚Äëalto y $\\alpha$ un t√©rmino de equilibrio.\n",
        "\n",
        "- CIoU es **diferenciable** y conduce a convergencia m√°s estable que IoU o GIoU.\n",
        "- YOLOv5 optimiza la posici√≥n de los *boxes* usando $L_\\text{CIoU} = 1 - \\text{CIoU}$.\n",
        "\n",
        "## 3.‚ÄØP√©rdida Binary¬†Cross¬†Entropy (BCE)\n",
        "Para un ejemplo con etiqueta binaria $y \\in \\{0,1\\}$ y probabilidad predicha $p$:\n",
        "\n",
        "$$\n",
        "\\text{BCE}(p, y) = -\\big(y \\log p + (1 - y) \\log(1 - p)\\big)\n",
        "$$\n",
        "\n",
        "- En YOLOv5 hay **dos instancias** de BCE:\n",
        "  1. **Objectness**: decide si existe objeto en cada celda‚Äëancla.\n",
        "  2. **Clasificaci√≥n**: distribuye probabilidad entre las $N$ clases (BCE por clase, activaci√≥n sigmoid).\n",
        "- BCE es adecuada para etiquetas escasas y balances asim√©tricos.\n",
        "\n",
        "## 4.‚ÄØFunci√≥n de p√©rdida total en YOLOv5\n",
        "La red minimiza una suma ponderada de tres componentes:\n",
        "\n",
        "$$\n",
        "\\mathcal{L}_\\text{total}\n",
        "= \\lambda_{\\text{box}}\\,\\mathcal{L}_{\\text{CIoU}}\n",
        "+ \\lambda_{\\text{obj}}\\,\\mathcal{L}_{\\text{BCE,obj}}\n",
        "+ \\lambda_{\\text{cls}}\\,\\mathcal{L}_{\\text{BCE,cls}}\n",
        "$$\n",
        "\n",
        "- $\\mathcal{L}_{\\text{CIoU}}$ ajusta posici√≥n y tama√±o de los *bounding boxes*.\n",
        "- $\\mathcal{L}_{\\text{BCE,obj}}$ refuerza la correcta predicci√≥n de presencia de objeto.\n",
        "- $\\mathcal{L}_{\\text{BCE,cls}}$ impulsa la clasificaci√≥n de la clase.\n",
        "- Los pesos $\\lambda$ se afinan emp√≠ricamente, t√≠picamente $\\approx (0.05, 1.0, 0.5)$, para equilibrar localizaci√≥n y reconocimiento.\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "A1aJXwJCFVtK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Actividad 5\n",
        "Investigue sobre el output de la red YOLOv5 y c√≥mo se traduce el tensor de salida a bounding boxes\n",
        "y detecciones de objetos. Adem√°s, explique c√≥mo, en general, se obtiene solamente una detecci√≥n\n",
        "por objeto, y no varias para todas las regiones donde el objeto est√° presente. Adem√°s, investigue el\n",
        "rol de la augmentaci√≥n de datos en el entrenamiento de YOLOv5."
      ],
      "metadata": {
        "id": "Fp33Qw3LJMRX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## 1.¬†Decodificaci√≥n del tensor de salida a *bounding boxes*\n",
        "YOLOv5 produce un tensor de forma `(B, A*(5+C), H, W)` donde:\n",
        "- *B* = batch, *A* = anclas por celda, *C* = n√∫mero de clases, *H√óW* = tama√±o de la malla.\n",
        "- Para cada ancla la red predice $\\big(t_x,\\,t_y,\\,t_w,\\,t_h,\\,p_{obj},\\,p_{cls,1\\dots C}\\big)$.\n",
        "\n",
        "Las coordenadas se decodifican con\n",
        "$$\n",
        "\\begin{aligned}\n",
        " x &= \\frac{\\sigma(t_x)+c_x}{W},\\\\\n",
        " y &= \\frac{\\sigma(t_y)+c_y}{H},\\\\\n",
        " w &= \\frac{e^{t_w}\\,a_w}{\\text{img}_w},\\\\\n",
        " h &= \\frac{e^{t_h}\\,a_h}{\\text{img}_h},\n",
        "\\end{aligned}\n",
        "$$\n",
        "donde $(c_x,c_y)$ son las coordenadas de la celda, $(a_w,a_h)$ las dimensiones del ancla y $\\sigma$ la sigmoide. As√≠ se obtienen *bounding boxes* normalizados $(x,y,w,h)$.\n",
        "\n",
        "## 2.¬†Una sola detecci√≥n por objeto: *Non‚ÄëMax Suppression* (NMS)\n",
        "- Despu√©s de decodificar, cada *box* lleva una confianza $p = \\sigma(p_{obj})\\max_j p_{cls,j}$.\n",
        "- Se descartan boxes con $p$ bajo (threshold).\n",
        "- Sobre los restantes se aplica **NMS**: se mantienen los boxes con mayor confianza y se eliminan los que tengan IoU $>\\tau$ con alguno ya aceptado.\n",
        "- YOLOv5 usa variantes como *CIoU‚ÄëNMS* o *Weighted NMS* seg√∫n la versi√≥n, pero el principio es id√©ntico: una sola predicci√≥n por objeto.\n",
        "\n",
        "## 3.¬†Rol del aumento de datos (*data augmentation*)\n",
        "El *augment* ampl√≠a la diversidad del set y mejora la generalizaci√≥n:\n",
        "- **Geometr√≠a**: escalado, recorte, volteo, rotaciones (√∫tiles cuando la orientaci√≥n del sat√©lite var√≠a).\n",
        "- **Color**: jitter HSV, blur, ruido.\n",
        "- **Mosaic & MixUp**: combinan cuatro im√°genes o mezclan pares, exponiendo m√∫ltiples objetos a cada contexto.\n",
        "- Beneficios principales: reduce *overfitting*, mejora robustez ante variaciones de iluminaci√≥n/escala y estabiliza el entrenamiento cuando los datos son escasos.\n",
        "\n",
        "En YOLOv5 el m√≥dulo `Albumentations` (o la implementaci√≥n interna de Ultralytics) integra estos *transforms* antes de pasar las im√°genes por la red.\n"
      ],
      "metadata": {
        "id": "ZxFgHXl5FJJS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Actividad 6\n",
        "En el notebook, deber√°n hacer fine-tuning para que una instancia de YOLOv5 pueda realizar seg-\n",
        "mentaciones de datos satelitales. Para esto, se encuentra implementado la carga del modelo YOLOv5\n",
        "\n",
        "con los par√°metros congelados salvo la cabeza de detecci√≥n. Se encuentra adem√°s el c√≥digo para\n",
        "cargar y preprocesar el set de datos. Se deben completar las siguientes funciones:"
      ],
      "metadata": {
        "id": "HZ7E6eC8Kiby"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TLKMzG0M3fTr",
        "outputId": "b3a95e48-c7d6-4dd8-a3c9-3754e640277b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting roboflow\n",
            "  Downloading roboflow-1.1.63-py3-none-any.whl.metadata (9.7 kB)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from roboflow) (2025.4.26)\n",
            "Collecting idna==3.7 (from roboflow)\n",
            "  Downloading idna-3.7-py3-none-any.whl.metadata (9.9 kB)\n",
            "Requirement already satisfied: cycler in /usr/local/lib/python3.11/dist-packages (from roboflow) (0.12.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from roboflow) (1.4.8)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from roboflow) (3.10.0)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.11/dist-packages (from roboflow) (2.0.2)\n",
            "Collecting opencv-python-headless==4.10.0.84 (from roboflow)\n",
            "  Downloading opencv_python_headless-4.10.0.84-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (20 kB)\n",
            "Requirement already satisfied: Pillow>=7.1.2 in /usr/local/lib/python3.11/dist-packages (from roboflow) (11.2.1)\n",
            "Collecting pillow-heif>=0.18.0 (from roboflow)\n",
            "  Downloading pillow_heif-0.22.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.6 kB)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.11/dist-packages (from roboflow) (2.9.0.post0)\n",
            "Collecting python-dotenv (from roboflow)\n",
            "  Downloading python_dotenv-1.1.0-py3-none-any.whl.metadata (24 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from roboflow) (2.32.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from roboflow) (1.17.0)\n",
            "Requirement already satisfied: urllib3>=1.26.6 in /usr/local/lib/python3.11/dist-packages (from roboflow) (2.4.0)\n",
            "Requirement already satisfied: tqdm>=4.41.0 in /usr/local/lib/python3.11/dist-packages (from roboflow) (4.67.1)\n",
            "Requirement already satisfied: PyYAML>=5.3.1 in /usr/local/lib/python3.11/dist-packages (from roboflow) (6.0.2)\n",
            "Requirement already satisfied: requests-toolbelt in /usr/local/lib/python3.11/dist-packages (from roboflow) (1.0.0)\n",
            "Collecting filetype (from roboflow)\n",
            "  Downloading filetype-1.2.0-py2.py3-none-any.whl.metadata (6.5 kB)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->roboflow) (1.3.2)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->roboflow) (4.57.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->roboflow) (24.2)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->roboflow) (3.2.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->roboflow) (3.4.1)\n",
            "Downloading roboflow-1.1.63-py3-none-any.whl (85 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m85.3/85.3 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading idna-3.7-py3-none-any.whl (66 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m66.8/66.8 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opencv_python_headless-4.10.0.84-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (49.9 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m49.9/49.9 MB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pillow_heif-0.22.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m24.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading filetype-1.2.0-py2.py3-none-any.whl (19 kB)\n",
            "Downloading python_dotenv-1.1.0-py3-none-any.whl (20 kB)\n",
            "Installing collected packages: filetype, python-dotenv, pillow-heif, opencv-python-headless, idna, roboflow\n",
            "  Attempting uninstall: opencv-python-headless\n",
            "    Found existing installation: opencv-python-headless 4.11.0.86\n",
            "    Uninstalling opencv-python-headless-4.11.0.86:\n",
            "      Successfully uninstalled opencv-python-headless-4.11.0.86\n",
            "  Attempting uninstall: idna\n",
            "    Found existing installation: idna 3.10\n",
            "    Uninstalling idna-3.10:\n",
            "      Successfully uninstalled idna-3.10\n",
            "Successfully installed filetype-1.2.0 idna-3.7 opencv-python-headless-4.10.0.84 pillow-heif-0.22.0 python-dotenv-1.1.0 roboflow-1.1.63\n",
            "loading Roboflow workspace...\n",
            "loading Roboflow project...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading Dataset Version Zip in Vehicle-Detection-1 to yolov5pytorch:: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 31811/31811 [00:00<00:00, 38850.89it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Extracting Dataset Version Zip to Vehicle-Detection-1 in yolov5pytorch:: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5528/5528 [00:01<00:00, 4726.26it/s]\n"
          ]
        }
      ],
      "source": [
        "!pip install roboflow\n",
        "# Para cargar un dataset desde Roboflow\n",
        "from roboflow import Roboflow\n",
        "my_key = \"hhJFVyqL27We1hzvzNAa\"\n",
        "rf = Roboflow(api_key=my_key)\n",
        "project = rf.workspace(\"cvproject-y6bf4\").project(\"vehicle-detection-gr77r\")\n",
        "version = project.version(1)\n",
        "dataset = version.download(\"yolov5\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ywGGeJWGhCmU",
        "outputId": "dcd24b14-5ada-40aa-b516-2217687a8c40"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'yolov5'...\n",
            "remote: Enumerating objects: 17430, done.\u001b[K\n",
            "remote: Counting objects: 100% (103/103), done.\u001b[K\n",
            "remote: Compressing objects: 100% (77/77), done.\u001b[K\n",
            "remote: Total 17430 (delta 73), reused 28 (delta 26), pack-reused 17327 (from 4)\u001b[K\n",
            "Receiving objects: 100% (17430/17430), 16.31 MiB | 7.76 MiB/s, done.\n",
            "Resolving deltas: 100% (11944/11944), done.\n",
            "/content/yolov5\n",
            "Requirement already satisfied: gitpython>=3.1.30 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 5)) (3.1.44)\n",
            "Requirement already satisfied: matplotlib>=3.3 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 6)) (3.10.0)\n",
            "Requirement already satisfied: numpy>=1.23.5 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 7)) (2.0.2)\n",
            "Requirement already satisfied: opencv-python>=4.1.1 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 8)) (4.11.0.86)\n",
            "Requirement already satisfied: pillow>=10.3.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 9)) (11.2.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 10)) (5.9.5)\n",
            "Requirement already satisfied: PyYAML>=5.3.1 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 11)) (6.0.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 12)) (2.32.3)\n",
            "Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 13)) (1.15.2)\n",
            "Collecting thop>=0.1.1 (from -r requirements.txt (line 14))\n",
            "  Downloading thop-0.1.1.post2209072238-py3-none-any.whl.metadata (2.7 kB)\n",
            "Requirement already satisfied: torch>=1.8.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 15)) (2.6.0+cu124)\n",
            "Requirement already satisfied: torchvision>=0.9.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 16)) (0.21.0+cu124)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 17)) (4.67.1)\n",
            "Collecting ultralytics>=8.2.34 (from -r requirements.txt (line 18))\n",
            "  Downloading ultralytics-8.3.128-py3-none-any.whl.metadata (37 kB)\n",
            "Requirement already satisfied: pandas>=1.1.4 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 27)) (2.2.2)\n",
            "Requirement already satisfied: seaborn>=0.11.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 28)) (0.13.2)\n",
            "Requirement already satisfied: setuptools>=70.0.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 42)) (75.2.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython>=3.1.30->-r requirements.txt (line 5)) (4.0.12)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3->-r requirements.txt (line 6)) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3->-r requirements.txt (line 6)) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3->-r requirements.txt (line 6)) (4.57.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3->-r requirements.txt (line 6)) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3->-r requirements.txt (line 6)) (24.2)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3->-r requirements.txt (line 6)) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3->-r requirements.txt (line 6)) (2.9.0.post0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->-r requirements.txt (line 12)) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->-r requirements.txt (line 12)) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->-r requirements.txt (line 12)) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->-r requirements.txt (line 12)) (2025.4.26)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->-r requirements.txt (line 15)) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->-r requirements.txt (line 15)) (4.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->-r requirements.txt (line 15)) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->-r requirements.txt (line 15)) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->-r requirements.txt (line 15)) (2025.3.2)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=1.8.0->-r requirements.txt (line 15))\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=1.8.0->-r requirements.txt (line 15))\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=1.8.0->-r requirements.txt (line 15))\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=1.8.0->-r requirements.txt (line 15))\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=1.8.0->-r requirements.txt (line 15))\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=1.8.0->-r requirements.txt (line 15))\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=1.8.0->-r requirements.txt (line 15))\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=1.8.0->-r requirements.txt (line 15))\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=1.8.0->-r requirements.txt (line 15))\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->-r requirements.txt (line 15)) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->-r requirements.txt (line 15)) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->-r requirements.txt (line 15)) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=1.8.0->-r requirements.txt (line 15))\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->-r requirements.txt (line 15)) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->-r requirements.txt (line 15)) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.8.0->-r requirements.txt (line 15)) (1.3.0)\n",
            "Requirement already satisfied: py-cpuinfo in /usr/local/lib/python3.11/dist-packages (from ultralytics>=8.2.34->-r requirements.txt (line 18)) (9.0.0)\n",
            "Collecting ultralytics-thop>=2.0.0 (from ultralytics>=8.2.34->-r requirements.txt (line 18))\n",
            "  Downloading ultralytics_thop-2.0.14-py3-none-any.whl.metadata (9.4 kB)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.1.4->-r requirements.txt (line 27)) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.1.4->-r requirements.txt (line 27)) (2025.2)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython>=3.1.30->-r requirements.txt (line 5)) (5.0.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib>=3.3->-r requirements.txt (line 6)) (1.17.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.8.0->-r requirements.txt (line 15)) (3.0.2)\n",
            "Downloading thop-0.1.1.post2209072238-py3-none-any.whl (15 kB)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m63.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m34.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m37.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m86.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ultralytics-8.3.128-py3-none-any.whl (1.0 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m42.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ultralytics_thop-2.0.14-py3-none-any.whl (26 kB)\n",
            "Installing collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, ultralytics-thop, thop, ultralytics\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 thop-0.1.1.post2209072238 ultralytics-8.3.128 ultralytics-thop-2.0.14\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.11/dist-packages (4.11.0.86)\n",
            "Requirement already satisfied: numpy>=1.21.2 in /usr/local/lib/python3.11/dist-packages (from opencv-python) (2.0.2)\n",
            "Collecting torchinfo\n",
            "  Downloading torchinfo-1.8.0-py3-none-any.whl.metadata (21 kB)\n",
            "Downloading torchinfo-1.8.0-py3-none-any.whl (23 kB)\n",
            "Installing collected packages: torchinfo\n",
            "Successfully installed torchinfo-1.8.0\n",
            "--2025-05-07 15:21:34--  https://github.com/ultralytics/yolov5/releases/download/v7.0/yolov5s.pt\n",
            "Resolving github.com (github.com)... 140.82.113.4\n",
            "Connecting to github.com (github.com)|140.82.113.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://objects.githubusercontent.com/github-production-release-asset-2e65be/264818686/381bd8a8-8910-4e9e-b0dd-2752951ef78c?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=releaseassetproduction%2F20250507%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20250507T152134Z&X-Amz-Expires=300&X-Amz-Signature=73affbd1b2113e97f0da0a963dd57f6dfa8c4d8ed3ac7bca9363578c2ad39b9b&X-Amz-SignedHeaders=host&response-content-disposition=attachment%3B%20filename%3Dyolov5s.pt&response-content-type=application%2Foctet-stream [following]\n",
            "--2025-05-07 15:21:34--  https://objects.githubusercontent.com/github-production-release-asset-2e65be/264818686/381bd8a8-8910-4e9e-b0dd-2752951ef78c?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=releaseassetproduction%2F20250507%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20250507T152134Z&X-Amz-Expires=300&X-Amz-Signature=73affbd1b2113e97f0da0a963dd57f6dfa8c4d8ed3ac7bca9363578c2ad39b9b&X-Amz-SignedHeaders=host&response-content-disposition=attachment%3B%20filename%3Dyolov5s.pt&response-content-type=application%2Foctet-stream\n",
            "Resolving objects.githubusercontent.com (objects.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to objects.githubusercontent.com (objects.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 14808437 (14M) [application/octet-stream]\n",
            "Saving to: ‚Äòyolov5s.pt‚Äô\n",
            "\n",
            "yolov5s.pt          100%[===================>]  14.12M  --.-KB/s    in 0.1s    \n",
            "\n",
            "2025-05-07 15:21:35 (114 MB/s) - ‚Äòyolov5s.pt‚Äô saved [14808437/14808437]\n",
            "\n",
            "/content\n"
          ]
        }
      ],
      "source": [
        "# Instalar dependencias de YOLOv5\n",
        "!git clone https://github.com/ultralytics/yolov5\n",
        "%cd yolov5\n",
        "!pip install -r requirements.txt\n",
        "!pip install opencv-python\n",
        "!pip install torchinfo\n",
        "!wget https://github.com/ultralytics/yolov5/releases/download/v7.0/yolov5s.pt\n",
        "%cd .."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "XVcxOFJp3mkw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c9695a55-af03-4b1d-9d3c-7b390d328ef8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating new Ultralytics Settings v0.0.6 file ‚úÖ \n",
            "View Ultralytics Settings with 'yolo settings' or at '/root/.config/Ultralytics/settings.json'\n",
            "Update Settings with 'yolo settings key=value', i.e. 'yolo settings runs_dir=path/to/dir'. For help see https://docs.ultralytics.com/quickstart/#ultralytics-settings.\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "sys.path.append('yolov5')\n",
        "import os\n",
        "import cv2\n",
        "import torch\n",
        "import numpy as np\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from PIL import Image\n",
        "from torchvision import transforms as T\n",
        "from models.yolo import Model, Detect\n",
        "import yaml\n",
        "from utils.loss import ComputeLoss\n",
        "from torchinfo import summary\n",
        "import albumentations as A"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "collapsed": true,
        "id": "OWd07IAW4ZJ9"
      },
      "outputs": [],
      "source": [
        "# Dataset de YOLO\n",
        "\n",
        "class YoloDataset(Dataset):\n",
        "    def __init__(self, root, img_size=640, augment=False):\n",
        "        self.image_dir = os.path.join(root, 'images')\n",
        "        self.label_dir = os.path.join(root, 'labels')\n",
        "        self.filenames = sorted(os.listdir(self.image_dir))[:2]\n",
        "        self.img_size = img_size\n",
        "\n",
        "        transforms =[\n",
        "              A.Resize(img_size, img_size),\n",
        "              A.Normalize(mean=(0.0, 0.0, 0.0), std=(1.0, 1.0, 1.0)),\n",
        "              A.ToTensorV2()\n",
        "              ]\n",
        "        if augment:\n",
        "            # Completar con rotaciones\n",
        "            # Completar con rotaciones\n",
        "            aug_transforms = [\n",
        "                A.RandomRotate90(p=0.5),  # Rotaci√≥n de 90 grados\n",
        "                A.Rotate(limit=30, p=0.7),  # Rotaci√≥n aleatoria hasta 30 grados\n",
        "                A.HorizontalFlip(p=0.5),    # Volteo horizontal\n",
        "                A.RandomBrightnessContrast(p=0.2)  # Ajustes de brillo/contraste\n",
        "            ]\n",
        "            transforms = aug_transforms + transforms\n",
        "\n",
        "\n",
        "        self.transform = A.Compose(transforms)\n",
        "        self.yaml_path = '/' + os.path.join(*root.split('/')[:-1], 'data.yaml')\n",
        "        with open(self.yaml_path, 'r') as f:\n",
        "            self.yaml = yaml.safe_load(f)\n",
        "        self.cls_to_idx = {cls: idx for idx, cls in enumerate(self.yaml['names'])}\n",
        "        self.idx_to_cls = {idx: cls for idx, cls in enumerate(self.yaml['names'])}\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_name = self.filenames[idx]\n",
        "        img_path = os.path.join(self.image_dir, img_name)\n",
        "        label_path = os.path.join(self.label_dir, img_name.replace('.jpg', '.txt').replace('.png', '.txt'))\n",
        "        img = np.array(Image.open(img_path).convert('RGB'))\n",
        "        labels = []\n",
        "        classes = []\n",
        "        if os.path.exists(label_path):\n",
        "            with open(label_path, 'r') as f:\n",
        "                for line in f:\n",
        "                    vals = list(map(float, line.strip().split()))\n",
        "                    if len(vals) >= 5:\n",
        "                        cls, bbox = int(vals[0]), vals[1:5]\n",
        "                        classes.append(cls)\n",
        "                        labels.append(bbox)\n",
        "        transformed = self.transform(image=img, bboxes=labels, class_labels=classes)\n",
        "        img = transformed['image']\n",
        "        boxes = torch.tensor(transformed['bboxes'], dtype=torch.float32)\n",
        "        classes = torch.tensor(transformed['class_labels'], dtype=torch.float32).unsqueeze(1)\n",
        "        if boxes.numel() > 0:\n",
        "            labels = torch.cat([classes, boxes], dim=1)\n",
        "        else:\n",
        "            labels = torch.zeros((0, 5), dtype=torch.float32)\n",
        "\n",
        "        return img, labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.filenames)\n",
        "\n",
        "# Convierte datos a formato YOLO\n",
        "def collate_fn(batch):\n",
        "    imgs, targets = [], []\n",
        "    for i, (img, target) in enumerate(batch):\n",
        "        imgs.append(img)\n",
        "        target = torch.cat([torch.full((target.size(0), 1), i), target], dim=1)\n",
        "        targets.append(target)\n",
        "\n",
        "    imgs = torch.stack(imgs, dim=0)\n",
        "    targets = torch.cat(targets, dim=0)\n",
        "\n",
        "    return imgs, targets\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "-yafJLA_WA7A",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fa2ab31f-2b7e-41a1-a526-b5006529cd94"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using cache found in /root/.cache/torch/hub/ultralytics_yolov5_master\n",
            "YOLOv5 üöÄ v7.0-416-gfe1d4d99 Python-3.11.12 torch-2.6.0+cu124 CPU\n",
            "\n",
            "Overriding model.yaml nc=80 with nc=4\n",
            "\n",
            "                 from  n    params  module                                  arguments                     \n",
            "  0                -1  1      3520  models.common.Conv                      [3, 32, 6, 2, 2]              \n",
            "  1                -1  1     18560  models.common.Conv                      [32, 64, 3, 2]                \n",
            "  2                -1  1     18816  models.common.C3                        [64, 64, 1]                   \n",
            "  3                -1  1     73984  models.common.Conv                      [64, 128, 3, 2]               \n",
            "  4                -1  2    115712  models.common.C3                        [128, 128, 2]                 \n",
            "  5                -1  1    295424  models.common.Conv                      [128, 256, 3, 2]              \n",
            "  6                -1  3    625152  models.common.C3                        [256, 256, 3]                 \n",
            "  7                -1  1   1180672  models.common.Conv                      [256, 512, 3, 2]              \n",
            "  8                -1  1   1182720  models.common.C3                        [512, 512, 1]                 \n",
            "  9                -1  1    656896  models.common.SPPF                      [512, 512, 5]                 \n",
            " 10                -1  1    131584  models.common.Conv                      [512, 256, 1, 1]              \n",
            " 11                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
            " 12           [-1, 6]  1         0  models.common.Concat                    [1]                           \n",
            " 13                -1  1    361984  models.common.C3                        [512, 256, 1, False]          \n",
            " 14                -1  1     33024  models.common.Conv                      [256, 128, 1, 1]              \n",
            " 15                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
            " 16           [-1, 4]  1         0  models.common.Concat                    [1]                           \n",
            " 17                -1  1     90880  models.common.C3                        [256, 128, 1, False]          \n",
            " 18                -1  1    147712  models.common.Conv                      [128, 128, 3, 2]              \n",
            " 19          [-1, 14]  1         0  models.common.Concat                    [1]                           \n",
            " 20                -1  1    296448  models.common.C3                        [256, 256, 1, False]          \n",
            " 21                -1  1    590336  models.common.Conv                      [256, 256, 3, 2]              \n",
            " 22          [-1, 10]  1         0  models.common.Concat                    [1]                           \n",
            " 23                -1  1   1182720  models.common.C3                        [512, 512, 1, False]          \n",
            " 24      [17, 20, 23]  1     24273  models.yolo.Detect                      [4, [[10, 13, 16, 30, 33, 23], [30, 61, 62, 45, 59, 119], [116, 90, 156, 198, 373, 326]], [128, 256, 512]]\n",
            "YOLOv5s summary: 214 layers, 7030417 parameters, 7030417 gradients, 16.0 GFLOPs\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import yaml\n",
        "from models.yolo import Model\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "train_dataset = YoloDataset('/content/Vehicle-Detection-1/train')\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True, collate_fn=collate_fn, num_workers = 2)\n",
        "\n",
        "\n",
        "model = torch.hub.load(\"ultralytics/yolov5\", \"yolov5s\", classes=len(train_dataset.yaml['names']), pretrained=True, autoshape=False).cpu()\n",
        "hyp_path = '/content/yolov5/data/hyps/hyp.scratch-low.yaml'\n",
        "\n",
        "with open(hyp_path) as f:\n",
        "    hyp = yaml.safe_load(f)\n",
        "\n",
        "# Congelar modelo\n",
        "for param in model.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "# Descongelar cabeza\n",
        "for param in model.model[-1].parameters():\n",
        "    param.requires_grad = True\n",
        "\n",
        "model = model.to('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model.hyp = hyp\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "DQwyDaPjVT2O"
      },
      "outputs": [],
      "source": [
        "from utils.loss import ComputeLoss\n",
        "\n",
        "def train(model, dataloader, optimizer, epochs):\n",
        "    model.train()\n",
        "    compute_loss = ComputeLoss(model)\n",
        "    history = []\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        epoch_loss = 0\n",
        "        epoch_box_loss = 0\n",
        "        epoch_obj_loss = 0\n",
        "        epoch_cls_loss = 0\n",
        "\n",
        "        for batch_i, (imgs, targets) in enumerate(dataloader):\n",
        "            imgs = imgs.to(device)\n",
        "            targets = targets.to(device)\n",
        "\n",
        "            pred = model(imgs)\n",
        "            loss, loss_items = compute_loss(pred, targets)\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            epoch_loss += loss.item()\n",
        "            epoch_box_loss += loss_items[0].item()\n",
        "            epoch_obj_loss += loss_items[1].item()\n",
        "            epoch_cls_loss += loss_items[2].item()\n",
        "\n",
        "        avg_loss = epoch_loss / len(dataloader)\n",
        "        avg_box_loss = epoch_box_loss / len(dataloader)\n",
        "        avg_obj_loss = epoch_obj_loss / len(dataloader)\n",
        "        avg_cls_loss = epoch_cls_loss / len(dataloader)\n",
        "\n",
        "        history.append({\n",
        "            'epoch': epoch + 1,\n",
        "            'loss': avg_loss,\n",
        "            'box_loss': avg_box_loss,\n",
        "            'obj_loss': avg_obj_loss,\n",
        "            'cls_loss': avg_cls_loss\n",
        "        })\n",
        "\n",
        "        print(f\"Epoch {epoch+1}/{epochs} - Loss: {avg_loss:.4f}, Box: {avg_box_loss:.4f}, Obj: {avg_obj_loss:.4f}, Cls: {avg_cls_loss:.4f}\")\n",
        "\n",
        "    return model, history\n",
        "\n",
        "def eval(model, dataloader, conf_thres=0.25, iou_thres=0.45):\n",
        "    model.eval()\n",
        "\n",
        "    stats = {\n",
        "        'images_processed': 0,\n",
        "        'total_detections': 0,\n",
        "        'detections_by_class': {},\n",
        "        'avg_confidence': 0,\n",
        "    }\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch_i, (imgs, targets) in enumerate(dataloader):\n",
        "            imgs = imgs.to(device)\n",
        "\n",
        "            output = model(imgs)\n",
        "            detections = output[0]\n",
        "\n",
        "            conf_mask = detections[..., 4] > conf_thres\n",
        "            detections = detections[conf_mask]\n",
        "\n",
        "            stats['images_processed'] += imgs.shape[0]\n",
        "\n",
        "            if detections.shape[0] > 0:\n",
        "                stats['total_detections'] += detections.shape[0]\n",
        "                stats['avg_confidence'] += detections[:, 4].sum().item()\n",
        "\n",
        "                for cls in detections[:, 5].cpu().numpy():\n",
        "                    cls_idx = int(cls)\n",
        "                    cls_name = train_dataset.idx_to_cls.get(cls_idx, f\"class_{cls_idx}\")\n",
        "                    stats['detections_by_class'][cls_name] = stats['detections_by_class'].get(cls_name, 0) + 1\n",
        "\n",
        "    if stats['total_detections'] > 0:\n",
        "        stats['avg_confidence'] /= stats['total_detections']\n",
        "\n",
        "    return stats"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Configuraci√≥n del optimizador\n",
        "optimizer = torch.optim.Adam(\n",
        "    filter(lambda p: p.requires_grad, model.parameters()),\n",
        "    lr=0.0001,\n",
        "    weight_decay=5e-4\n",
        ")\n",
        "\n",
        "# Datasets\n",
        "train_dataset = YoloDataset('/content/Vehicle-Detection-1/train')\n",
        "val_dataset = YoloDataset('/content/Vehicle-Detection-1/valid')\n",
        "\n",
        "# Dataloaders\n",
        "train_loader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=4,\n",
        "    shuffle=True,\n",
        "    collate_fn=collate_fn,\n",
        "    num_workers=2\n",
        ")\n",
        "\n",
        "val_loader = DataLoader(\n",
        "    val_dataset,\n",
        "    batch_size=4,\n",
        "    shuffle=False,\n",
        "    collate_fn=collate_fn,\n",
        "    num_workers=2\n",
        ")\n",
        "\n",
        "# Entrenamiento\n",
        "trained_model, history = train(model, train_loader, optimizer, epochs=5)\n",
        "\n",
        "# Evaluaci√≥n\n",
        "eval_stats = eval(trained_model, val_loader)\n",
        "\n",
        "# Guardar modelo\n",
        "torch.save(trained_model.state_dict(), 'yolov5_finetuned.pt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-AKDp5OTOG93",
        "outputId": "183aa668-2e6d-4ae5-b749-095fa20a4e35"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5 - Loss: 2.2405, Box: 0.1284, Obj: 0.0267, Cls: 0.9652\n",
            "Epoch 2/5 - Loss: 2.2223, Box: 0.1277, Obj: 0.0265, Cls: 0.9569\n",
            "Epoch 3/5 - Loss: 2.2042, Box: 0.1270, Obj: 0.0264, Cls: 0.9487\n",
            "Epoch 4/5 - Loss: 2.1862, Box: 0.1263, Obj: 0.0263, Cls: 0.9405\n",
            "Epoch 5/5 - Loss: 2.1684, Box: 0.1256, Obj: 0.0262, Cls: 0.9324\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Actividad 7\n",
        "Finalmente, realice fine-tuning de dos modelos YOLOv5: uno sin transformaciones de datos adicionales y otro utilizando rotaciones, es decir, aplicando aumento de datos a partir de versiones\n",
        "\n",
        "rotadas de las im√°genes originales. ¬øSe observa alguna diferencia notable en su rendimiento? ¬øPor\n",
        "qu√© las rotaciones constituyen una buena estrategia de aumento de datos para este problema?\n",
        "Elija 3 im√°genes representativas del set de validaci√≥n y muestre los bounding boxes encontrados por\n",
        "cada modelo."
      ],
      "metadata": {
        "id": "tdW1n3YbNGUo"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "EodnNvmdPrWt",
        "outputId": "dc96849f-46af-4976-b9c0-6d67fb9e090b"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'yolov5s_finetuned.pt'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-c3b95045ffd3>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorchvision\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnms\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'yolov5s_finetuned.pt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'matplotlib'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'inline'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/ultralytics/utils/patches.py\u001b[0m in \u001b[0;36mtorch_load\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"weights_only\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_torch_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1423\u001b[0m         \u001b[0mpickle_load_args\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"encoding\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1424\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1425\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1426\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_is_zipfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1427\u001b[0m             \u001b[0;31m# The zipfile reader is going to advance the current file position.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    749\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    750\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_is_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 751\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    752\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    753\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m\"w\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    730\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_opener\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    731\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 732\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    733\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    734\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'yolov5s_finetuned.pt'"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.patches import Rectangle\n",
        "from torchvision.ops import nms\n",
        "\n",
        "model.load_state_dict(torch.load('yolov5s_finetuned.pt'))\n",
        "%matplotlib inline\n",
        "\n",
        "# Set model to eval\n",
        "model.eval().to(device)\n",
        "\n",
        "# Get one test sample\n",
        "img_tensor, labels = train_dataset[0] # image: [3, H, W], labels: [N, 5]\n",
        "img_np = img_tensor.permute(1, 2, 0).numpy()\n",
        "H, W = img_np.shape[:2]\n",
        "\n",
        "# Set up plot\n",
        "fig, ax = plt.subplots(1, figsize=(8, 8))\n",
        "ax.imshow(img_np)\n",
        "# ax.set_title(\"Ground Truth (Green) vs Predictions (Red)\")\n",
        "\n",
        "# # ---- Plot Ground Truth ----\n",
        "for label in labels:\n",
        "    cls = int(label[0].item())\n",
        "    x_center, y_center, width, height = label[1:] * torch.tensor([W, H, W, H])\n",
        "    x1 = (x_center - width / 2).item()\n",
        "    y1 = (y_center - height / 2).item()\n",
        "\n",
        "    # Rectangle for ground truth\n",
        "    rect = Rectangle((x1, y1), width.item(), height.item(),\n",
        "                     linewidth=2, edgecolor='lime', facecolor='none')\n",
        "    ax.add_patch(rect)\n",
        "\n",
        "# ---- Predictions ----\n",
        "\n",
        "# Completar\n",
        "# ---- Predictions ----\n",
        "\n",
        "# Obtener predicciones del modelo\n",
        "with torch.no_grad():\n",
        "    model.eval()\n",
        "    img_batch = img_tensor.unsqueeze(0).to(device)  # A√±adir dimensi√≥n de batch\n",
        "    predictions = model(img_batch)\n",
        "\n",
        "pred = predictions[0].cpu()  # Extraer predicciones del batch\n",
        "\n",
        "# Aplicar filtro de confianza\n",
        "conf_threshold = 0.25\n",
        "conf_mask = pred[:, 4] > conf_threshold\n",
        "pred = pred[conf_mask]\n",
        "\n",
        "# Dibujar las predicciones\n",
        "if len(pred) > 0:\n",
        "    for detection in pred:\n",
        "        x_center, y_center, width, height = detection[:4]\n",
        "        conf = detection[4].item()\n",
        "        cls_idx = int(detection[5].item())\n",
        "        cls_name = train_dataset.idx_to_cls.get(cls_idx, f\"class_{cls_idx}\")\n",
        "\n",
        "        # Convertir a coordenadas de p√≠xeles\n",
        "        x_center, y_center = x_center * W, y_center * H\n",
        "        width, height = width * W, height * H\n",
        "\n",
        "        # Calcular esquina superior izquierda\n",
        "        x1 = (x_center - width / 2).item()\n",
        "        y1 = (y_center - height / 2).item()\n",
        "\n",
        "        # Dibujar rect√°ngulo para predicci√≥n\n",
        "        rect = Rectangle((x1, y1), width.item(), height.item(),\n",
        "                        linewidth=2, edgecolor='red', facecolor='none')\n",
        "        ax.add_patch(rect)\n",
        "\n",
        "        # A√±adir etiqueta con clase y confianza\n",
        "        label_text = f\"{cls_name}: {conf:.2f}\"\n",
        "        ax.text(x1, y1-5, label_text, color='white', fontsize=8,\n",
        "                bbox=dict(facecolor='red', alpha=0.5))\n",
        "else:\n",
        "    ax.text(W/2, 20, \"No se detectaron objetos\", color='red', fontsize=12,\n",
        "           horizontalalignment='center')\n",
        "\n",
        "plt.title(\"Ground Truth (Verde) vs Predicciones (Rojo)\")\n",
        "plt.tight_layout()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Actividad 7: Comparaci√≥n de modelos con y sin aumento de datos\n",
        "\n",
        "# 1. Crear datasets para entrenamiento con y sin aumento\n",
        "train_dataset_no_aug = YoloDataset('/content/Vehicle-Detection-1/train', augment=False)\n",
        "train_dataset_with_aug = YoloDataset('/content/Vehicle-Detection-1/train', augment=True)\n",
        "val_dataset = YoloDataset('/content/Vehicle-Detection-1/valid', augment=False)\n",
        "\n",
        "# 2. Crear dataloaders\n",
        "train_loader_no_aug = DataLoader(\n",
        "    train_dataset_no_aug,\n",
        "    batch_size=4,\n",
        "    shuffle=True,\n",
        "    collate_fn=collate_fn,\n",
        "    num_workers=2\n",
        ")\n",
        "\n",
        "train_loader_with_aug = DataLoader(\n",
        "    train_dataset_with_aug,\n",
        "    batch_size=4,\n",
        "    shuffle=True,\n",
        "    collate_fn=collate_fn,\n",
        "    num_workers=2\n",
        ")\n",
        "\n",
        "val_loader = DataLoader(\n",
        "    val_dataset,\n",
        "    batch_size=4,\n",
        "    shuffle=False,\n",
        "    collate_fn=collate_fn,\n",
        "    num_workers=2\n",
        ")\n",
        "\n",
        "# 3. Inicializar el primer modelo (sin aumento)\n",
        "model_no_aug = torch.hub.load(\"ultralytics/yolov5\", \"yolov5s\",\n",
        "                              classes=len(train_dataset_no_aug.yaml['names']),\n",
        "                              pretrained=True, autoshape=False).cpu()\n",
        "\n",
        "# Congelar todas las capas excepto la cabeza\n",
        "for param in model_no_aug.parameters():\n",
        "    param.requires_grad = False\n",
        "for param in model_no_aug.model[-1].parameters():\n",
        "    param.requires_grad = True\n",
        "\n",
        "model_no_aug = model_no_aug.to(device)\n",
        "model_no_aug.hyp = hyp\n",
        "\n",
        "optimizer_no_aug = torch.optim.Adam(\n",
        "    filter(lambda p: p.requires_grad, model_no_aug.parameters()),\n",
        "    lr=0.0001,\n",
        "    weight_decay=5e-4\n",
        ")\n",
        "\n",
        "# 4. Entrenar modelo sin aumento\n",
        "print(\"Entrenando modelo sin aumento de datos:\")\n",
        "model_no_aug, history_no_aug = train(model_no_aug, train_loader_no_aug, optimizer_no_aug, epochs=5)\n",
        "torch.save(model_no_aug.state_dict(), 'yolov5_no_aug.pt')\n",
        "\n",
        "# 5. Inicializar el segundo modelo (con aumento)\n",
        "model_with_aug = torch.hub.load(\"ultralytics/yolov5\", \"yolov5s\",\n",
        "                               classes=len(train_dataset_with_aug.yaml['names']),\n",
        "                               pretrained=True, autoshape=False).cpu()\n",
        "\n",
        "# Congelar todas las capas excepto la cabeza\n",
        "for param in model_with_aug.parameters():\n",
        "    param.requires_grad = False\n",
        "for param in model_with_aug.model[-1].parameters():\n",
        "    param.requires_grad = True\n",
        "\n",
        "model_with_aug = model_with_aug.to(device)\n",
        "model_with_aug.hyp = hyp\n",
        "\n",
        "optimizer_with_aug = torch.optim.Adam(\n",
        "    filter(lambda p: p.requires_grad, model_with_aug.parameters()),\n",
        "    lr=0.0001,\n",
        "    weight_decay=5e-4\n",
        ")\n",
        "\n",
        "# 6. Entrenar modelo con aumento\n",
        "print(\"Entrenando modelo con aumento de datos (rotaciones):\")\n",
        "model_with_aug, history_with_aug = train(model_with_aug, train_loader_with_aug, optimizer_with_aug, epochs=5)\n",
        "torch.save(model_with_aug.state_dict(), 'yolov5_with_aug.pt')\n",
        "\n",
        "# 7. Evaluar ambos modelos en el conjunto de validaci√≥n\n",
        "print(\"Evaluando modelo sin aumento:\")\n",
        "stats_no_aug = eval(model_no_aug, val_loader)\n",
        "print(stats_no_aug)\n",
        "\n",
        "print(\"Evaluando modelo con aumento:\")\n",
        "stats_with_aug = eval(model_with_aug, val_loader)\n",
        "print(stats_with_aug)\n",
        "\n",
        "# 8. Funci√≥n para visualizar y comparar detecciones\n",
        "def visualize_comparison(img_idx):\n",
        "    \"\"\"\n",
        "    Visualiza y compara las predicciones de ambos modelos en una imagen\n",
        "    \"\"\"\n",
        "    img_tensor, labels = val_dataset[img_idx]\n",
        "    img_np = img_tensor.permute(1, 2, 0).numpy()\n",
        "    H, W = img_np.shape[:2]\n",
        "\n",
        "    # Normalizar imagen si es necesario\n",
        "    img_np = (img_np - img_np.min()) / (img_np.max() - img_np.min())\n",
        "\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(16, 8))\n",
        "\n",
        "    # T√≠tulos para cada subgr√°fico\n",
        "    titles = [\"Modelo sin aumento\", \"Modelo con rotaciones\"]\n",
        "    models = [model_no_aug, model_with_aug]\n",
        "\n",
        "    for i, (ax, model, title) in enumerate(zip(axes, models, titles)):\n",
        "        ax.imshow(img_np)\n",
        "        ax.set_title(title)\n",
        "\n",
        "        # Dibujar ground truth (verde)\n",
        "        for label in labels:\n",
        "            cls = int(label[0].item())\n",
        "            cls_name = val_dataset.idx_to_cls.get(cls, f\"class_{cls}\")\n",
        "            x_center, y_center, width, height = label[1:] * torch.tensor([W, H, W, H])\n",
        "            x1 = (x_center - width / 2).item()\n",
        "            y1 = (y_center - height / 2).item()\n",
        "\n",
        "            # Rect√°ngulo para ground truth\n",
        "            rect = Rectangle((x1, y1), width.item(), height.item(),\n",
        "                             linewidth=2, edgecolor='lime', facecolor='none')\n",
        "            ax.add_patch(rect)\n",
        "            ax.text(x1, y1-5, cls_name, color='white', fontsize=8,\n",
        "                    bbox=dict(facecolor='green', alpha=0.5))\n",
        "\n",
        "        # Obtener predicciones\n",
        "        with torch.no_grad():\n",
        "            model.eval()\n",
        "            img_batch = img_tensor.unsqueeze(0).to(device)\n",
        "            predictions = model(img_batch)\n",
        "            pred = predictions[0].cpu()\n",
        "\n",
        "        # Filtrar predicciones por confianza\n",
        "        conf_threshold = 0.25\n",
        "        conf_mask = pred[:, 4] > conf_threshold\n",
        "        pred = pred[conf_mask]\n",
        "\n",
        "        # Dibujar predicciones (rojo)\n",
        "        for detection in pred:\n",
        "            x_center, y_center, width, height = detection[:4]\n",
        "            conf = detection[4].item()\n",
        "            cls_idx = int(detection[5].item())\n",
        "            cls_name = val_dataset.idx_to_cls.get(cls_idx, f\"class_{cls_idx}\")\n",
        "\n",
        "            # Convertir a coordenadas de p√≠xeles\n",
        "            x_center, y_center = x_center * W, y_center * H\n",
        "            width, height = width * W, height * H\n",
        "            x1 = (x_center - width / 2).item()\n",
        "            y1 = (y_center - height / 2).item()\n",
        "\n",
        "            # Rect√°ngulo para predicci√≥n\n",
        "            rect = Rectangle((x1, y1), width.item(), height.item(),\n",
        "                             linewidth=2, edgecolor='red', facecolor='none')\n",
        "            ax.add_patch(rect)\n",
        "            ax.text(x1, y1+height.item()+5, f\"{cls_name}: {conf:.2f}\",\n",
        "                    color='white', fontsize=8,\n",
        "                    bbox=dict(facecolor='red', alpha=0.5))\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(f'comparison_image_{img_idx}.png')\n",
        "    plt.show()\n",
        "\n",
        "# 9. Visualizar 3 im√°genes representativas\n",
        "print(\"Visualizando comparaciones en im√°genes representativas:\")\n",
        "for idx in [0, 5, 10]:  # Puedes ajustar estos √≠ndices seg√∫n necesites\n",
        "    if idx < len(val_dataset):\n",
        "        print(f\"Imagen {idx}:\")\n",
        "        visualize_comparison(idx)"
      ],
      "metadata": {
        "id": "oxBGvIK9U62U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "r2Pk1UgQXaO-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}