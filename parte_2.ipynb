{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNmCZAFpcmdgyctYsWVNYVm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fuentes145/Tarea2/blob/main/parte_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BL-oYKcKICDz"
      },
      "source": [
        "# 2. Finetuning YOLOv5: ``You-Only-Look-Once\""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Actividad 4\n",
        "Investigue sobre la arquitectura de los modelos YOLO y explique los siguientes conceptos (para cada\n",
        "caso use solo 5 líneas máximo):\n",
        "\n",
        "\n",
        "• Diferencia entre frameworks de detección de objetos de una y dos etapas (single-stage vs.\n",
        "two-stage object detection).\n",
        "\n",
        "\n",
        "• Función de pérdida Complete Intersection Over Union (CIoU) y su uso en la red YOLOv5.\n",
        "\n",
        "\n",
        "• Función de pérdida Binary Cross Entropy (BCE) y su uso en la red YOLOv5.\n",
        "\n",
        "• ¿Cómo se calcula la función de pérdida total en YOLOv5?"
      ],
      "metadata": {
        "id": "yG6_FdclFGxg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "## 1. Single‑stage vs. Two‑stage object detection\n",
        "- **Modelos de una etapa (single‑stage)**, como YOLO, *predicen directamente* bounding boxes + clases en un **único paso** sobre la imagen, obteniendo gran velocidad.\n",
        "- **Modelos de dos etapas (two‑stage)**, como Faster R‑CNN, primero generan **regiones propuestas (RPN)** y luego **refinan y clasifican** cada propuesta.\n",
        "- El enfoque de una etapa sacrifica algo de exactitud por latencia bajísima; el de dos etapas logra mejor mAP pero con mayor coste computacional.\n",
        "- En tareas de *streaming* o tiempo real, la relación *precisión/velocidad* de YOLO lo hace preferible.\n",
        "\n",
        "## 2. Pérdida Complete IoU (CIoU)\n",
        "La métrica CIoU extiende la IoU clásica penalizando también la distancia entre centros y la discrepancia de aspecto:\n",
        "\n",
        "$$\n",
        "\\text{CIoU}(b, b^{gt}) = \\text{IoU}\n",
        "- \\frac{\\rho^2(\\mathbf{c}, \\mathbf{c^{gt}})}{c^2}\n",
        "- \\alpha v\n",
        "$$\n",
        "\n",
        "con $\\rho$ la distancia euclídea entre centros, $c$ la diagonal del cuadro mínimo que los contiene, $v$ la divergencia de relación ancho‑alto y $\\alpha$ un término de equilibrio.\n",
        "\n",
        "- CIoU es **diferenciable** y conduce a convergencia más estable que IoU o GIoU.\n",
        "- YOLOv5 optimiza la posición de los *boxes* usando $L_\\text{CIoU} = 1 - \\text{CIoU}$.\n",
        "\n",
        "## 3. Pérdida Binary Cross Entropy (BCE)\n",
        "Para un ejemplo con etiqueta binaria $y \\in \\{0,1\\}$ y probabilidad predicha $p$:\n",
        "\n",
        "$$\n",
        "\\text{BCE}(p, y) = -\\big(y \\log p + (1 - y) \\log(1 - p)\\big)\n",
        "$$\n",
        "\n",
        "- En YOLOv5 hay **dos instancias** de BCE:\n",
        "  1. **Objectness**: decide si existe objeto en cada celda‑ancla.\n",
        "  2. **Clasificación**: distribuye probabilidad entre las $N$ clases (BCE por clase, activación sigmoid).\n",
        "- BCE es adecuada para etiquetas escasas y balances asimétricos.\n",
        "\n",
        "## 4. Función de pérdida total en YOLOv5\n",
        "La red minimiza una suma ponderada de tres componentes:\n",
        "\n",
        "$$\n",
        "\\mathcal{L}_\\text{total}\n",
        "= \\lambda_{\\text{box}}\\,\\mathcal{L}_{\\text{CIoU}}\n",
        "+ \\lambda_{\\text{obj}}\\,\\mathcal{L}_{\\text{BCE,obj}}\n",
        "+ \\lambda_{\\text{cls}}\\,\\mathcal{L}_{\\text{BCE,cls}}\n",
        "$$\n",
        "\n",
        "- $\\mathcal{L}_{\\text{CIoU}}$ ajusta posición y tamaño de los *bounding boxes*.\n",
        "- $\\mathcal{L}_{\\text{BCE,obj}}$ refuerza la correcta predicción de presencia de objeto.\n",
        "- $\\mathcal{L}_{\\text{BCE,cls}}$ impulsa la clasificación de la clase.\n",
        "- Los pesos $\\lambda$ se afinan empíricamente, típicamente $\\approx (0.05, 1.0, 0.5)$, para equilibrar localización y reconocimiento.\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "A1aJXwJCFVtK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Actividad 5\n",
        "Investigue sobre el output de la red YOLOv5 y cómo se traduce el tensor de salida a bounding boxes\n",
        "y detecciones de objetos. Además, explique cómo, en general, se obtiene solamente una detección\n",
        "por objeto, y no varias para todas las regiones donde el objeto está presente. Además, investigue el\n",
        "rol de la augmentación de datos en el entrenamiento de YOLOv5."
      ],
      "metadata": {
        "id": "Fp33Qw3LJMRX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## 1. Decodificación del tensor de salida a *bounding boxes*\n",
        "YOLOv5 produce un tensor de forma `(B, A*(5+C), H, W)` donde:\n",
        "- *B* = batch, *A* = anclas por celda, *C* = número de clases, *H×W* = tamaño de la malla.\n",
        "- Para cada ancla la red predice $\\big(t_x,\\,t_y,\\,t_w,\\,t_h,\\,p_{obj},\\,p_{cls,1\\dots C}\\big)$.\n",
        "\n",
        "Las coordenadas se decodifican con\n",
        "$$\n",
        "\\begin{aligned}\n",
        " x &= \\frac{\\sigma(t_x)+c_x}{W},\\\\\n",
        " y &= \\frac{\\sigma(t_y)+c_y}{H},\\\\\n",
        " w &= \\frac{e^{t_w}\\,a_w}{\\text{img}_w},\\\\\n",
        " h &= \\frac{e^{t_h}\\,a_h}{\\text{img}_h},\n",
        "\\end{aligned}\n",
        "$$\n",
        "donde $(c_x,c_y)$ son las coordenadas de la celda, $(a_w,a_h)$ las dimensiones del ancla y $\\sigma$ la sigmoide. Así se obtienen *bounding boxes* normalizados $(x,y,w,h)$.\n",
        "\n",
        "## 2. Una sola detección por objeto: *Non‑Max Suppression* (NMS)\n",
        "- Después de decodificar, cada *box* lleva una confianza $p = \\sigma(p_{obj})\\max_j p_{cls,j}$.\n",
        "- Se descartan boxes con $p$ bajo (threshold).\n",
        "- Sobre los restantes se aplica **NMS**: se mantienen los boxes con mayor confianza y se eliminan los que tengan IoU $>\\tau$ con alguno ya aceptado.\n",
        "- YOLOv5 usa variantes como *CIoU‑NMS* o *Weighted NMS* según la versión, pero el principio es idéntico: una sola predicción por objeto.\n",
        "\n",
        "## 3. Rol del aumento de datos (*data augmentation*)\n",
        "El *augment* amplía la diversidad del set y mejora la generalización:\n",
        "- **Geometría**: escalado, recorte, volteo, rotaciones (útiles cuando la orientación del satélite varía).\n",
        "- **Color**: jitter HSV, blur, ruido.\n",
        "- **Mosaic & MixUp**: combinan cuatro imágenes o mezclan pares, exponiendo múltiples objetos a cada contexto.\n",
        "- Beneficios principales: reduce *overfitting*, mejora robustez ante variaciones de iluminación/escala y estabiliza el entrenamiento cuando los datos son escasos.\n",
        "\n",
        "En YOLOv5 el módulo `Albumentations` (o la implementación interna de Ultralytics) integra estos *transforms* antes de pasar las imágenes por la red.\n"
      ],
      "metadata": {
        "id": "ZxFgHXl5FJJS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Actividad 6\n",
        "En el notebook, deberán hacer fine-tuning para que una instancia de YOLOv5 pueda realizar seg-\n",
        "mentaciones de datos satelitales. Para esto, se encuentra implementado la carga del modelo YOLOv5\n",
        "\n",
        "con los parámetros congelados salvo la cabeza de detección. Se encuentra además el código para\n",
        "cargar y preprocesar el set de datos. Se deben completar las siguientes funciones:"
      ],
      "metadata": {
        "id": "HZ7E6eC8Kiby"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TLKMzG0M3fTr",
        "outputId": "b3a95e48-c7d6-4dd8-a3c9-3754e640277b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting roboflow\n",
            "  Downloading roboflow-1.1.63-py3-none-any.whl.metadata (9.7 kB)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from roboflow) (2025.4.26)\n",
            "Collecting idna==3.7 (from roboflow)\n",
            "  Downloading idna-3.7-py3-none-any.whl.metadata (9.9 kB)\n",
            "Requirement already satisfied: cycler in /usr/local/lib/python3.11/dist-packages (from roboflow) (0.12.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from roboflow) (1.4.8)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from roboflow) (3.10.0)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.11/dist-packages (from roboflow) (2.0.2)\n",
            "Collecting opencv-python-headless==4.10.0.84 (from roboflow)\n",
            "  Downloading opencv_python_headless-4.10.0.84-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (20 kB)\n",
            "Requirement already satisfied: Pillow>=7.1.2 in /usr/local/lib/python3.11/dist-packages (from roboflow) (11.2.1)\n",
            "Collecting pillow-heif>=0.18.0 (from roboflow)\n",
            "  Downloading pillow_heif-0.22.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.6 kB)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.11/dist-packages (from roboflow) (2.9.0.post0)\n",
            "Collecting python-dotenv (from roboflow)\n",
            "  Downloading python_dotenv-1.1.0-py3-none-any.whl.metadata (24 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from roboflow) (2.32.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from roboflow) (1.17.0)\n",
            "Requirement already satisfied: urllib3>=1.26.6 in /usr/local/lib/python3.11/dist-packages (from roboflow) (2.4.0)\n",
            "Requirement already satisfied: tqdm>=4.41.0 in /usr/local/lib/python3.11/dist-packages (from roboflow) (4.67.1)\n",
            "Requirement already satisfied: PyYAML>=5.3.1 in /usr/local/lib/python3.11/dist-packages (from roboflow) (6.0.2)\n",
            "Requirement already satisfied: requests-toolbelt in /usr/local/lib/python3.11/dist-packages (from roboflow) (1.0.0)\n",
            "Collecting filetype (from roboflow)\n",
            "  Downloading filetype-1.2.0-py2.py3-none-any.whl.metadata (6.5 kB)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->roboflow) (1.3.2)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->roboflow) (4.57.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->roboflow) (24.2)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->roboflow) (3.2.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->roboflow) (3.4.1)\n",
            "Downloading roboflow-1.1.63-py3-none-any.whl (85 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.3/85.3 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading idna-3.7-py3-none-any.whl (66 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.8/66.8 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opencv_python_headless-4.10.0.84-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (49.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.9/49.9 MB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pillow_heif-0.22.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m24.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading filetype-1.2.0-py2.py3-none-any.whl (19 kB)\n",
            "Downloading python_dotenv-1.1.0-py3-none-any.whl (20 kB)\n",
            "Installing collected packages: filetype, python-dotenv, pillow-heif, opencv-python-headless, idna, roboflow\n",
            "  Attempting uninstall: opencv-python-headless\n",
            "    Found existing installation: opencv-python-headless 4.11.0.86\n",
            "    Uninstalling opencv-python-headless-4.11.0.86:\n",
            "      Successfully uninstalled opencv-python-headless-4.11.0.86\n",
            "  Attempting uninstall: idna\n",
            "    Found existing installation: idna 3.10\n",
            "    Uninstalling idna-3.10:\n",
            "      Successfully uninstalled idna-3.10\n",
            "Successfully installed filetype-1.2.0 idna-3.7 opencv-python-headless-4.10.0.84 pillow-heif-0.22.0 python-dotenv-1.1.0 roboflow-1.1.63\n",
            "loading Roboflow workspace...\n",
            "loading Roboflow project...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading Dataset Version Zip in Vehicle-Detection-1 to yolov5pytorch:: 100%|██████████| 31811/31811 [00:00<00:00, 38850.89it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Extracting Dataset Version Zip to Vehicle-Detection-1 in yolov5pytorch:: 100%|██████████| 5528/5528 [00:01<00:00, 4726.26it/s]\n"
          ]
        }
      ],
      "source": [
        "!pip install roboflow\n",
        "# Para cargar un dataset desde Roboflow\n",
        "from roboflow import Roboflow\n",
        "my_key = \"hhJFVyqL27We1hzvzNAa\"\n",
        "rf = Roboflow(api_key=my_key)\n",
        "project = rf.workspace(\"cvproject-y6bf4\").project(\"vehicle-detection-gr77r\")\n",
        "version = project.version(1)\n",
        "dataset = version.download(\"yolov5\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ywGGeJWGhCmU",
        "outputId": "dcd24b14-5ada-40aa-b516-2217687a8c40"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'yolov5'...\n",
            "remote: Enumerating objects: 17430, done.\u001b[K\n",
            "remote: Counting objects: 100% (103/103), done.\u001b[K\n",
            "remote: Compressing objects: 100% (77/77), done.\u001b[K\n",
            "remote: Total 17430 (delta 73), reused 28 (delta 26), pack-reused 17327 (from 4)\u001b[K\n",
            "Receiving objects: 100% (17430/17430), 16.31 MiB | 7.76 MiB/s, done.\n",
            "Resolving deltas: 100% (11944/11944), done.\n",
            "/content/yolov5\n",
            "Requirement already satisfied: gitpython>=3.1.30 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 5)) (3.1.44)\n",
            "Requirement already satisfied: matplotlib>=3.3 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 6)) (3.10.0)\n",
            "Requirement already satisfied: numpy>=1.23.5 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 7)) (2.0.2)\n",
            "Requirement already satisfied: opencv-python>=4.1.1 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 8)) (4.11.0.86)\n",
            "Requirement already satisfied: pillow>=10.3.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 9)) (11.2.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 10)) (5.9.5)\n",
            "Requirement already satisfied: PyYAML>=5.3.1 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 11)) (6.0.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 12)) (2.32.3)\n",
            "Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 13)) (1.15.2)\n",
            "Collecting thop>=0.1.1 (from -r requirements.txt (line 14))\n",
            "  Downloading thop-0.1.1.post2209072238-py3-none-any.whl.metadata (2.7 kB)\n",
            "Requirement already satisfied: torch>=1.8.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 15)) (2.6.0+cu124)\n",
            "Requirement already satisfied: torchvision>=0.9.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 16)) (0.21.0+cu124)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 17)) (4.67.1)\n",
            "Collecting ultralytics>=8.2.34 (from -r requirements.txt (line 18))\n",
            "  Downloading ultralytics-8.3.128-py3-none-any.whl.metadata (37 kB)\n",
            "Requirement already satisfied: pandas>=1.1.4 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 27)) (2.2.2)\n",
            "Requirement already satisfied: seaborn>=0.11.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 28)) (0.13.2)\n",
            "Requirement already satisfied: setuptools>=70.0.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 42)) (75.2.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython>=3.1.30->-r requirements.txt (line 5)) (4.0.12)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3->-r requirements.txt (line 6)) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3->-r requirements.txt (line 6)) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3->-r requirements.txt (line 6)) (4.57.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3->-r requirements.txt (line 6)) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3->-r requirements.txt (line 6)) (24.2)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3->-r requirements.txt (line 6)) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3->-r requirements.txt (line 6)) (2.9.0.post0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->-r requirements.txt (line 12)) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->-r requirements.txt (line 12)) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->-r requirements.txt (line 12)) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->-r requirements.txt (line 12)) (2025.4.26)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->-r requirements.txt (line 15)) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->-r requirements.txt (line 15)) (4.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->-r requirements.txt (line 15)) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->-r requirements.txt (line 15)) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->-r requirements.txt (line 15)) (2025.3.2)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=1.8.0->-r requirements.txt (line 15))\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=1.8.0->-r requirements.txt (line 15))\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=1.8.0->-r requirements.txt (line 15))\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=1.8.0->-r requirements.txt (line 15))\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=1.8.0->-r requirements.txt (line 15))\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=1.8.0->-r requirements.txt (line 15))\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=1.8.0->-r requirements.txt (line 15))\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=1.8.0->-r requirements.txt (line 15))\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=1.8.0->-r requirements.txt (line 15))\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->-r requirements.txt (line 15)) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->-r requirements.txt (line 15)) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->-r requirements.txt (line 15)) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=1.8.0->-r requirements.txt (line 15))\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->-r requirements.txt (line 15)) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->-r requirements.txt (line 15)) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.8.0->-r requirements.txt (line 15)) (1.3.0)\n",
            "Requirement already satisfied: py-cpuinfo in /usr/local/lib/python3.11/dist-packages (from ultralytics>=8.2.34->-r requirements.txt (line 18)) (9.0.0)\n",
            "Collecting ultralytics-thop>=2.0.0 (from ultralytics>=8.2.34->-r requirements.txt (line 18))\n",
            "  Downloading ultralytics_thop-2.0.14-py3-none-any.whl.metadata (9.4 kB)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.1.4->-r requirements.txt (line 27)) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.1.4->-r requirements.txt (line 27)) (2025.2)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython>=3.1.30->-r requirements.txt (line 5)) (5.0.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib>=3.3->-r requirements.txt (line 6)) (1.17.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.8.0->-r requirements.txt (line 15)) (3.0.2)\n",
            "Downloading thop-0.1.1.post2209072238-py3-none-any.whl (15 kB)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m63.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m34.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m37.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m86.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ultralytics-8.3.128-py3-none-any.whl (1.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m42.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ultralytics_thop-2.0.14-py3-none-any.whl (26 kB)\n",
            "Installing collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, ultralytics-thop, thop, ultralytics\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 thop-0.1.1.post2209072238 ultralytics-8.3.128 ultralytics-thop-2.0.14\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.11/dist-packages (4.11.0.86)\n",
            "Requirement already satisfied: numpy>=1.21.2 in /usr/local/lib/python3.11/dist-packages (from opencv-python) (2.0.2)\n",
            "Collecting torchinfo\n",
            "  Downloading torchinfo-1.8.0-py3-none-any.whl.metadata (21 kB)\n",
            "Downloading torchinfo-1.8.0-py3-none-any.whl (23 kB)\n",
            "Installing collected packages: torchinfo\n",
            "Successfully installed torchinfo-1.8.0\n",
            "--2025-05-07 15:21:34--  https://github.com/ultralytics/yolov5/releases/download/v7.0/yolov5s.pt\n",
            "Resolving github.com (github.com)... 140.82.113.4\n",
            "Connecting to github.com (github.com)|140.82.113.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://objects.githubusercontent.com/github-production-release-asset-2e65be/264818686/381bd8a8-8910-4e9e-b0dd-2752951ef78c?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=releaseassetproduction%2F20250507%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20250507T152134Z&X-Amz-Expires=300&X-Amz-Signature=73affbd1b2113e97f0da0a963dd57f6dfa8c4d8ed3ac7bca9363578c2ad39b9b&X-Amz-SignedHeaders=host&response-content-disposition=attachment%3B%20filename%3Dyolov5s.pt&response-content-type=application%2Foctet-stream [following]\n",
            "--2025-05-07 15:21:34--  https://objects.githubusercontent.com/github-production-release-asset-2e65be/264818686/381bd8a8-8910-4e9e-b0dd-2752951ef78c?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=releaseassetproduction%2F20250507%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20250507T152134Z&X-Amz-Expires=300&X-Amz-Signature=73affbd1b2113e97f0da0a963dd57f6dfa8c4d8ed3ac7bca9363578c2ad39b9b&X-Amz-SignedHeaders=host&response-content-disposition=attachment%3B%20filename%3Dyolov5s.pt&response-content-type=application%2Foctet-stream\n",
            "Resolving objects.githubusercontent.com (objects.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to objects.githubusercontent.com (objects.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 14808437 (14M) [application/octet-stream]\n",
            "Saving to: ‘yolov5s.pt’\n",
            "\n",
            "yolov5s.pt          100%[===================>]  14.12M  --.-KB/s    in 0.1s    \n",
            "\n",
            "2025-05-07 15:21:35 (114 MB/s) - ‘yolov5s.pt’ saved [14808437/14808437]\n",
            "\n",
            "/content\n"
          ]
        }
      ],
      "source": [
        "# Instalar dependencias de YOLOv5\n",
        "!git clone https://github.com/ultralytics/yolov5\n",
        "%cd yolov5\n",
        "!pip install -r requirements.txt\n",
        "!pip install opencv-python\n",
        "!pip install torchinfo\n",
        "!wget https://github.com/ultralytics/yolov5/releases/download/v7.0/yolov5s.pt\n",
        "%cd .."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "XVcxOFJp3mkw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c9695a55-af03-4b1d-9d3c-7b390d328ef8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating new Ultralytics Settings v0.0.6 file ✅ \n",
            "View Ultralytics Settings with 'yolo settings' or at '/root/.config/Ultralytics/settings.json'\n",
            "Update Settings with 'yolo settings key=value', i.e. 'yolo settings runs_dir=path/to/dir'. For help see https://docs.ultralytics.com/quickstart/#ultralytics-settings.\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "sys.path.append('yolov5')\n",
        "import os\n",
        "import cv2\n",
        "import torch\n",
        "import numpy as np\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from PIL import Image\n",
        "from torchvision import transforms as T\n",
        "from models.yolo import Model, Detect\n",
        "import yaml\n",
        "from utils.loss import ComputeLoss\n",
        "from torchinfo import summary\n",
        "import albumentations as A"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "collapsed": true,
        "id": "OWd07IAW4ZJ9"
      },
      "outputs": [],
      "source": [
        "# Dataset de YOLO\n",
        "\n",
        "class YoloDataset(Dataset):\n",
        "    def __init__(self, root, img_size=640, augment=False):\n",
        "        self.image_dir = os.path.join(root, 'images')\n",
        "        self.label_dir = os.path.join(root, 'labels')\n",
        "        self.filenames = sorted(os.listdir(self.image_dir))[:2]\n",
        "        self.img_size = img_size\n",
        "\n",
        "        transforms =[\n",
        "              A.Resize(img_size, img_size),\n",
        "              A.Normalize(mean=(0.0, 0.0, 0.0), std=(1.0, 1.0, 1.0)),\n",
        "              A.ToTensorV2()\n",
        "              ]\n",
        "        if augment:\n",
        "            # Completar con rotaciones\n",
        "            # Completar con rotaciones\n",
        "            aug_transforms = [\n",
        "                A.RandomRotate90(p=0.5),  # Rotación de 90 grados\n",
        "                A.Rotate(limit=30, p=0.7),  # Rotación aleatoria hasta 30 grados\n",
        "                A.HorizontalFlip(p=0.5),    # Volteo horizontal\n",
        "                A.RandomBrightnessContrast(p=0.2)  # Ajustes de brillo/contraste\n",
        "            ]\n",
        "            transforms = aug_transforms + transforms\n",
        "\n",
        "\n",
        "        self.transform = A.Compose(transforms)\n",
        "        self.yaml_path = '/' + os.path.join(*root.split('/')[:-1], 'data.yaml')\n",
        "        with open(self.yaml_path, 'r') as f:\n",
        "            self.yaml = yaml.safe_load(f)\n",
        "        self.cls_to_idx = {cls: idx for idx, cls in enumerate(self.yaml['names'])}\n",
        "        self.idx_to_cls = {idx: cls for idx, cls in enumerate(self.yaml['names'])}\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_name = self.filenames[idx]\n",
        "        img_path = os.path.join(self.image_dir, img_name)\n",
        "        label_path = os.path.join(self.label_dir, img_name.replace('.jpg', '.txt').replace('.png', '.txt'))\n",
        "        img = np.array(Image.open(img_path).convert('RGB'))\n",
        "        labels = []\n",
        "        classes = []\n",
        "        if os.path.exists(label_path):\n",
        "            with open(label_path, 'r') as f:\n",
        "                for line in f:\n",
        "                    vals = list(map(float, line.strip().split()))\n",
        "                    if len(vals) >= 5:\n",
        "                        cls, bbox = int(vals[0]), vals[1:5]\n",
        "                        classes.append(cls)\n",
        "                        labels.append(bbox)\n",
        "        transformed = self.transform(image=img, bboxes=labels, class_labels=classes)\n",
        "        img = transformed['image']\n",
        "        boxes = torch.tensor(transformed['bboxes'], dtype=torch.float32)\n",
        "        classes = torch.tensor(transformed['class_labels'], dtype=torch.float32).unsqueeze(1)\n",
        "        if boxes.numel() > 0:\n",
        "            labels = torch.cat([classes, boxes], dim=1)\n",
        "        else:\n",
        "            labels = torch.zeros((0, 5), dtype=torch.float32)\n",
        "\n",
        "        return img, labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.filenames)\n",
        "\n",
        "# Convierte datos a formato YOLO\n",
        "def collate_fn(batch):\n",
        "    imgs, targets = [], []\n",
        "    for i, (img, target) in enumerate(batch):\n",
        "        imgs.append(img)\n",
        "        target = torch.cat([torch.full((target.size(0), 1), i), target], dim=1)\n",
        "        targets.append(target)\n",
        "\n",
        "    imgs = torch.stack(imgs, dim=0)\n",
        "    targets = torch.cat(targets, dim=0)\n",
        "\n",
        "    return imgs, targets\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "-yafJLA_WA7A",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fa2ab31f-2b7e-41a1-a526-b5006529cd94"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using cache found in /root/.cache/torch/hub/ultralytics_yolov5_master\n",
            "YOLOv5 🚀 v7.0-416-gfe1d4d99 Python-3.11.12 torch-2.6.0+cu124 CPU\n",
            "\n",
            "Overriding model.yaml nc=80 with nc=4\n",
            "\n",
            "                 from  n    params  module                                  arguments                     \n",
            "  0                -1  1      3520  models.common.Conv                      [3, 32, 6, 2, 2]              \n",
            "  1                -1  1     18560  models.common.Conv                      [32, 64, 3, 2]                \n",
            "  2                -1  1     18816  models.common.C3                        [64, 64, 1]                   \n",
            "  3                -1  1     73984  models.common.Conv                      [64, 128, 3, 2]               \n",
            "  4                -1  2    115712  models.common.C3                        [128, 128, 2]                 \n",
            "  5                -1  1    295424  models.common.Conv                      [128, 256, 3, 2]              \n",
            "  6                -1  3    625152  models.common.C3                        [256, 256, 3]                 \n",
            "  7                -1  1   1180672  models.common.Conv                      [256, 512, 3, 2]              \n",
            "  8                -1  1   1182720  models.common.C3                        [512, 512, 1]                 \n",
            "  9                -1  1    656896  models.common.SPPF                      [512, 512, 5]                 \n",
            " 10                -1  1    131584  models.common.Conv                      [512, 256, 1, 1]              \n",
            " 11                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
            " 12           [-1, 6]  1         0  models.common.Concat                    [1]                           \n",
            " 13                -1  1    361984  models.common.C3                        [512, 256, 1, False]          \n",
            " 14                -1  1     33024  models.common.Conv                      [256, 128, 1, 1]              \n",
            " 15                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
            " 16           [-1, 4]  1         0  models.common.Concat                    [1]                           \n",
            " 17                -1  1     90880  models.common.C3                        [256, 128, 1, False]          \n",
            " 18                -1  1    147712  models.common.Conv                      [128, 128, 3, 2]              \n",
            " 19          [-1, 14]  1         0  models.common.Concat                    [1]                           \n",
            " 20                -1  1    296448  models.common.C3                        [256, 256, 1, False]          \n",
            " 21                -1  1    590336  models.common.Conv                      [256, 256, 3, 2]              \n",
            " 22          [-1, 10]  1         0  models.common.Concat                    [1]                           \n",
            " 23                -1  1   1182720  models.common.C3                        [512, 512, 1, False]          \n",
            " 24      [17, 20, 23]  1     24273  models.yolo.Detect                      [4, [[10, 13, 16, 30, 33, 23], [30, 61, 62, 45, 59, 119], [116, 90, 156, 198, 373, 326]], [128, 256, 512]]\n",
            "YOLOv5s summary: 214 layers, 7030417 parameters, 7030417 gradients, 16.0 GFLOPs\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import yaml\n",
        "from models.yolo import Model\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "train_dataset = YoloDataset('/content/Vehicle-Detection-1/train')\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True, collate_fn=collate_fn, num_workers = 2)\n",
        "\n",
        "\n",
        "model = torch.hub.load(\"ultralytics/yolov5\", \"yolov5s\", classes=len(train_dataset.yaml['names']), pretrained=True, autoshape=False).cpu()\n",
        "hyp_path = '/content/yolov5/data/hyps/hyp.scratch-low.yaml'\n",
        "\n",
        "with open(hyp_path) as f:\n",
        "    hyp = yaml.safe_load(f)\n",
        "\n",
        "# Congelar modelo\n",
        "for param in model.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "# Descongelar cabeza\n",
        "for param in model.model[-1].parameters():\n",
        "    param.requires_grad = True\n",
        "\n",
        "model = model.to('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model.hyp = hyp\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "DQwyDaPjVT2O"
      },
      "outputs": [],
      "source": [
        "from utils.loss import ComputeLoss\n",
        "\n",
        "def train(model, dataloader, optimizer, epochs):\n",
        "    model.train()\n",
        "    compute_loss = ComputeLoss(model)\n",
        "    history = []\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        epoch_loss = 0\n",
        "        epoch_box_loss = 0\n",
        "        epoch_obj_loss = 0\n",
        "        epoch_cls_loss = 0\n",
        "\n",
        "        for batch_i, (imgs, targets) in enumerate(dataloader):\n",
        "            imgs = imgs.to(device)\n",
        "            targets = targets.to(device)\n",
        "\n",
        "            pred = model(imgs)\n",
        "            loss, loss_items = compute_loss(pred, targets)\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            epoch_loss += loss.item()\n",
        "            epoch_box_loss += loss_items[0].item()\n",
        "            epoch_obj_loss += loss_items[1].item()\n",
        "            epoch_cls_loss += loss_items[2].item()\n",
        "\n",
        "        avg_loss = epoch_loss / len(dataloader)\n",
        "        avg_box_loss = epoch_box_loss / len(dataloader)\n",
        "        avg_obj_loss = epoch_obj_loss / len(dataloader)\n",
        "        avg_cls_loss = epoch_cls_loss / len(dataloader)\n",
        "\n",
        "        history.append({\n",
        "            'epoch': epoch + 1,\n",
        "            'loss': avg_loss,\n",
        "            'box_loss': avg_box_loss,\n",
        "            'obj_loss': avg_obj_loss,\n",
        "            'cls_loss': avg_cls_loss\n",
        "        })\n",
        "\n",
        "        print(f\"Epoch {epoch+1}/{epochs} - Loss: {avg_loss:.4f}, Box: {avg_box_loss:.4f}, Obj: {avg_obj_loss:.4f}, Cls: {avg_cls_loss:.4f}\")\n",
        "\n",
        "    return model, history\n",
        "\n",
        "def eval(model, dataloader, conf_thres=0.25, iou_thres=0.45):\n",
        "    model.eval()\n",
        "\n",
        "    stats = {\n",
        "        'images_processed': 0,\n",
        "        'total_detections': 0,\n",
        "        'detections_by_class': {},\n",
        "        'avg_confidence': 0,\n",
        "    }\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch_i, (imgs, targets) in enumerate(dataloader):\n",
        "            imgs = imgs.to(device)\n",
        "\n",
        "            output = model(imgs)\n",
        "            detections = output[0]\n",
        "\n",
        "            conf_mask = detections[..., 4] > conf_thres\n",
        "            detections = detections[conf_mask]\n",
        "\n",
        "            stats['images_processed'] += imgs.shape[0]\n",
        "\n",
        "            if detections.shape[0] > 0:\n",
        "                stats['total_detections'] += detections.shape[0]\n",
        "                stats['avg_confidence'] += detections[:, 4].sum().item()\n",
        "\n",
        "                for cls in detections[:, 5].cpu().numpy():\n",
        "                    cls_idx = int(cls)\n",
        "                    cls_name = train_dataset.idx_to_cls.get(cls_idx, f\"class_{cls_idx}\")\n",
        "                    stats['detections_by_class'][cls_name] = stats['detections_by_class'].get(cls_name, 0) + 1\n",
        "\n",
        "    if stats['total_detections'] > 0:\n",
        "        stats['avg_confidence'] /= stats['total_detections']\n",
        "\n",
        "    return stats"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Configuración del optimizador\n",
        "optimizer = torch.optim.Adam(\n",
        "    filter(lambda p: p.requires_grad, model.parameters()),\n",
        "    lr=0.0001,\n",
        "    weight_decay=5e-4\n",
        ")\n",
        "\n",
        "# Datasets\n",
        "train_dataset = YoloDataset('/content/Vehicle-Detection-1/train')\n",
        "val_dataset = YoloDataset('/content/Vehicle-Detection-1/valid')\n",
        "\n",
        "# Dataloaders\n",
        "train_loader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=4,\n",
        "    shuffle=True,\n",
        "    collate_fn=collate_fn,\n",
        "    num_workers=2\n",
        ")\n",
        "\n",
        "val_loader = DataLoader(\n",
        "    val_dataset,\n",
        "    batch_size=4,\n",
        "    shuffle=False,\n",
        "    collate_fn=collate_fn,\n",
        "    num_workers=2\n",
        ")\n",
        "\n",
        "# Entrenamiento\n",
        "trained_model, history = train(model, train_loader, optimizer, epochs=5)\n",
        "\n",
        "# Evaluación\n",
        "eval_stats = eval(trained_model, val_loader)\n",
        "\n",
        "# Guardar modelo\n",
        "torch.save(trained_model.state_dict(), 'yolov5_finetuned.pt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-AKDp5OTOG93",
        "outputId": "183aa668-2e6d-4ae5-b749-095fa20a4e35"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5 - Loss: 2.2405, Box: 0.1284, Obj: 0.0267, Cls: 0.9652\n",
            "Epoch 2/5 - Loss: 2.2223, Box: 0.1277, Obj: 0.0265, Cls: 0.9569\n",
            "Epoch 3/5 - Loss: 2.2042, Box: 0.1270, Obj: 0.0264, Cls: 0.9487\n",
            "Epoch 4/5 - Loss: 2.1862, Box: 0.1263, Obj: 0.0263, Cls: 0.9405\n",
            "Epoch 5/5 - Loss: 2.1684, Box: 0.1256, Obj: 0.0262, Cls: 0.9324\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Actividad 7\n",
        "Finalmente, realice fine-tuning de dos modelos YOLOv5: uno sin transformaciones de datos adicionales y otro utilizando rotaciones, es decir, aplicando aumento de datos a partir de versiones\n",
        "\n",
        "rotadas de las imágenes originales. ¿Se observa alguna diferencia notable en su rendimiento? ¿Por\n",
        "qué las rotaciones constituyen una buena estrategia de aumento de datos para este problema?\n",
        "Elija 3 imágenes representativas del set de validación y muestre los bounding boxes encontrados por\n",
        "cada modelo."
      ],
      "metadata": {
        "id": "tdW1n3YbNGUo"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "EodnNvmdPrWt",
        "outputId": "dc96849f-46af-4976-b9c0-6d67fb9e090b"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'yolov5s_finetuned.pt'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-c3b95045ffd3>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorchvision\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnms\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'yolov5s_finetuned.pt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'matplotlib'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'inline'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/ultralytics/utils/patches.py\u001b[0m in \u001b[0;36mtorch_load\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"weights_only\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_torch_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1423\u001b[0m         \u001b[0mpickle_load_args\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"encoding\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1424\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1425\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1426\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_is_zipfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1427\u001b[0m             \u001b[0;31m# The zipfile reader is going to advance the current file position.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    749\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    750\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_is_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 751\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    752\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    753\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m\"w\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    730\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_opener\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    731\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 732\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    733\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    734\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'yolov5s_finetuned.pt'"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.patches import Rectangle\n",
        "from torchvision.ops import nms\n",
        "\n",
        "model.load_state_dict(torch.load('yolov5s_finetuned.pt'))\n",
        "%matplotlib inline\n",
        "\n",
        "# Set model to eval\n",
        "model.eval().to(device)\n",
        "\n",
        "# Get one test sample\n",
        "img_tensor, labels = train_dataset[0] # image: [3, H, W], labels: [N, 5]\n",
        "img_np = img_tensor.permute(1, 2, 0).numpy()\n",
        "H, W = img_np.shape[:2]\n",
        "\n",
        "# Set up plot\n",
        "fig, ax = plt.subplots(1, figsize=(8, 8))\n",
        "ax.imshow(img_np)\n",
        "# ax.set_title(\"Ground Truth (Green) vs Predictions (Red)\")\n",
        "\n",
        "# # ---- Plot Ground Truth ----\n",
        "for label in labels:\n",
        "    cls = int(label[0].item())\n",
        "    x_center, y_center, width, height = label[1:] * torch.tensor([W, H, W, H])\n",
        "    x1 = (x_center - width / 2).item()\n",
        "    y1 = (y_center - height / 2).item()\n",
        "\n",
        "    # Rectangle for ground truth\n",
        "    rect = Rectangle((x1, y1), width.item(), height.item(),\n",
        "                     linewidth=2, edgecolor='lime', facecolor='none')\n",
        "    ax.add_patch(rect)\n",
        "\n",
        "# ---- Predictions ----\n",
        "\n",
        "# Completar\n",
        "# ---- Predictions ----\n",
        "\n",
        "# Obtener predicciones del modelo\n",
        "with torch.no_grad():\n",
        "    model.eval()\n",
        "    img_batch = img_tensor.unsqueeze(0).to(device)  # Añadir dimensión de batch\n",
        "    predictions = model(img_batch)\n",
        "\n",
        "pred = predictions[0].cpu()  # Extraer predicciones del batch\n",
        "\n",
        "# Aplicar filtro de confianza\n",
        "conf_threshold = 0.25\n",
        "conf_mask = pred[:, 4] > conf_threshold\n",
        "pred = pred[conf_mask]\n",
        "\n",
        "# Dibujar las predicciones\n",
        "if len(pred) > 0:\n",
        "    for detection in pred:\n",
        "        x_center, y_center, width, height = detection[:4]\n",
        "        conf = detection[4].item()\n",
        "        cls_idx = int(detection[5].item())\n",
        "        cls_name = train_dataset.idx_to_cls.get(cls_idx, f\"class_{cls_idx}\")\n",
        "\n",
        "        # Convertir a coordenadas de píxeles\n",
        "        x_center, y_center = x_center * W, y_center * H\n",
        "        width, height = width * W, height * H\n",
        "\n",
        "        # Calcular esquina superior izquierda\n",
        "        x1 = (x_center - width / 2).item()\n",
        "        y1 = (y_center - height / 2).item()\n",
        "\n",
        "        # Dibujar rectángulo para predicción\n",
        "        rect = Rectangle((x1, y1), width.item(), height.item(),\n",
        "                        linewidth=2, edgecolor='red', facecolor='none')\n",
        "        ax.add_patch(rect)\n",
        "\n",
        "        # Añadir etiqueta con clase y confianza\n",
        "        label_text = f\"{cls_name}: {conf:.2f}\"\n",
        "        ax.text(x1, y1-5, label_text, color='white', fontsize=8,\n",
        "                bbox=dict(facecolor='red', alpha=0.5))\n",
        "else:\n",
        "    ax.text(W/2, 20, \"No se detectaron objetos\", color='red', fontsize=12,\n",
        "           horizontalalignment='center')\n",
        "\n",
        "plt.title(\"Ground Truth (Verde) vs Predicciones (Rojo)\")\n",
        "plt.tight_layout()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Actividad 7: Comparación de modelos con y sin aumento de datos\n",
        "\n",
        "# 1. Crear datasets para entrenamiento con y sin aumento\n",
        "train_dataset_no_aug = YoloDataset('/content/Vehicle-Detection-1/train', augment=False)\n",
        "train_dataset_with_aug = YoloDataset('/content/Vehicle-Detection-1/train', augment=True)\n",
        "val_dataset = YoloDataset('/content/Vehicle-Detection-1/valid', augment=False)\n",
        "\n",
        "# 2. Crear dataloaders\n",
        "train_loader_no_aug = DataLoader(\n",
        "    train_dataset_no_aug,\n",
        "    batch_size=4,\n",
        "    shuffle=True,\n",
        "    collate_fn=collate_fn,\n",
        "    num_workers=2\n",
        ")\n",
        "\n",
        "train_loader_with_aug = DataLoader(\n",
        "    train_dataset_with_aug,\n",
        "    batch_size=4,\n",
        "    shuffle=True,\n",
        "    collate_fn=collate_fn,\n",
        "    num_workers=2\n",
        ")\n",
        "\n",
        "val_loader = DataLoader(\n",
        "    val_dataset,\n",
        "    batch_size=4,\n",
        "    shuffle=False,\n",
        "    collate_fn=collate_fn,\n",
        "    num_workers=2\n",
        ")\n",
        "\n",
        "# 3. Inicializar el primer modelo (sin aumento)\n",
        "model_no_aug = torch.hub.load(\"ultralytics/yolov5\", \"yolov5s\",\n",
        "                              classes=len(train_dataset_no_aug.yaml['names']),\n",
        "                              pretrained=True, autoshape=False).cpu()\n",
        "\n",
        "# Congelar todas las capas excepto la cabeza\n",
        "for param in model_no_aug.parameters():\n",
        "    param.requires_grad = False\n",
        "for param in model_no_aug.model[-1].parameters():\n",
        "    param.requires_grad = True\n",
        "\n",
        "model_no_aug = model_no_aug.to(device)\n",
        "model_no_aug.hyp = hyp\n",
        "\n",
        "optimizer_no_aug = torch.optim.Adam(\n",
        "    filter(lambda p: p.requires_grad, model_no_aug.parameters()),\n",
        "    lr=0.0001,\n",
        "    weight_decay=5e-4\n",
        ")\n",
        "\n",
        "# 4. Entrenar modelo sin aumento\n",
        "print(\"Entrenando modelo sin aumento de datos:\")\n",
        "model_no_aug, history_no_aug = train(model_no_aug, train_loader_no_aug, optimizer_no_aug, epochs=5)\n",
        "torch.save(model_no_aug.state_dict(), 'yolov5_no_aug.pt')\n",
        "\n",
        "# 5. Inicializar el segundo modelo (con aumento)\n",
        "model_with_aug = torch.hub.load(\"ultralytics/yolov5\", \"yolov5s\",\n",
        "                               classes=len(train_dataset_with_aug.yaml['names']),\n",
        "                               pretrained=True, autoshape=False).cpu()\n",
        "\n",
        "# Congelar todas las capas excepto la cabeza\n",
        "for param in model_with_aug.parameters():\n",
        "    param.requires_grad = False\n",
        "for param in model_with_aug.model[-1].parameters():\n",
        "    param.requires_grad = True\n",
        "\n",
        "model_with_aug = model_with_aug.to(device)\n",
        "model_with_aug.hyp = hyp\n",
        "\n",
        "optimizer_with_aug = torch.optim.Adam(\n",
        "    filter(lambda p: p.requires_grad, model_with_aug.parameters()),\n",
        "    lr=0.0001,\n",
        "    weight_decay=5e-4\n",
        ")\n",
        "\n",
        "# 6. Entrenar modelo con aumento\n",
        "print(\"Entrenando modelo con aumento de datos (rotaciones):\")\n",
        "model_with_aug, history_with_aug = train(model_with_aug, train_loader_with_aug, optimizer_with_aug, epochs=5)\n",
        "torch.save(model_with_aug.state_dict(), 'yolov5_with_aug.pt')\n",
        "\n",
        "# 7. Evaluar ambos modelos en el conjunto de validación\n",
        "print(\"Evaluando modelo sin aumento:\")\n",
        "stats_no_aug = eval(model_no_aug, val_loader)\n",
        "print(stats_no_aug)\n",
        "\n",
        "print(\"Evaluando modelo con aumento:\")\n",
        "stats_with_aug = eval(model_with_aug, val_loader)\n",
        "print(stats_with_aug)\n",
        "\n",
        "# 8. Función para visualizar y comparar detecciones\n",
        "def visualize_comparison(img_idx):\n",
        "    \"\"\"\n",
        "    Visualiza y compara las predicciones de ambos modelos en una imagen\n",
        "    \"\"\"\n",
        "    img_tensor, labels = val_dataset[img_idx]\n",
        "    img_np = img_tensor.permute(1, 2, 0).numpy()\n",
        "    H, W = img_np.shape[:2]\n",
        "\n",
        "    # Normalizar imagen si es necesario\n",
        "    img_np = (img_np - img_np.min()) / (img_np.max() - img_np.min())\n",
        "\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(16, 8))\n",
        "\n",
        "    # Títulos para cada subgráfico\n",
        "    titles = [\"Modelo sin aumento\", \"Modelo con rotaciones\"]\n",
        "    models = [model_no_aug, model_with_aug]\n",
        "\n",
        "    for i, (ax, model, title) in enumerate(zip(axes, models, titles)):\n",
        "        ax.imshow(img_np)\n",
        "        ax.set_title(title)\n",
        "\n",
        "        # Dibujar ground truth (verde)\n",
        "        for label in labels:\n",
        "            cls = int(label[0].item())\n",
        "            cls_name = val_dataset.idx_to_cls.get(cls, f\"class_{cls}\")\n",
        "            x_center, y_center, width, height = label[1:] * torch.tensor([W, H, W, H])\n",
        "            x1 = (x_center - width / 2).item()\n",
        "            y1 = (y_center - height / 2).item()\n",
        "\n",
        "            # Rectángulo para ground truth\n",
        "            rect = Rectangle((x1, y1), width.item(), height.item(),\n",
        "                             linewidth=2, edgecolor='lime', facecolor='none')\n",
        "            ax.add_patch(rect)\n",
        "            ax.text(x1, y1-5, cls_name, color='white', fontsize=8,\n",
        "                    bbox=dict(facecolor='green', alpha=0.5))\n",
        "\n",
        "        # Obtener predicciones\n",
        "        with torch.no_grad():\n",
        "            model.eval()\n",
        "            img_batch = img_tensor.unsqueeze(0).to(device)\n",
        "            predictions = model(img_batch)\n",
        "            pred = predictions[0].cpu()\n",
        "\n",
        "        # Filtrar predicciones por confianza\n",
        "        conf_threshold = 0.25\n",
        "        conf_mask = pred[:, 4] > conf_threshold\n",
        "        pred = pred[conf_mask]\n",
        "\n",
        "        # Dibujar predicciones (rojo)\n",
        "        for detection in pred:\n",
        "            x_center, y_center, width, height = detection[:4]\n",
        "            conf = detection[4].item()\n",
        "            cls_idx = int(detection[5].item())\n",
        "            cls_name = val_dataset.idx_to_cls.get(cls_idx, f\"class_{cls_idx}\")\n",
        "\n",
        "            # Convertir a coordenadas de píxeles\n",
        "            x_center, y_center = x_center * W, y_center * H\n",
        "            width, height = width * W, height * H\n",
        "            x1 = (x_center - width / 2).item()\n",
        "            y1 = (y_center - height / 2).item()\n",
        "\n",
        "            # Rectángulo para predicción\n",
        "            rect = Rectangle((x1, y1), width.item(), height.item(),\n",
        "                             linewidth=2, edgecolor='red', facecolor='none')\n",
        "            ax.add_patch(rect)\n",
        "            ax.text(x1, y1+height.item()+5, f\"{cls_name}: {conf:.2f}\",\n",
        "                    color='white', fontsize=8,\n",
        "                    bbox=dict(facecolor='red', alpha=0.5))\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(f'comparison_image_{img_idx}.png')\n",
        "    plt.show()\n",
        "\n",
        "# 9. Visualizar 3 imágenes representativas\n",
        "print(\"Visualizando comparaciones en imágenes representativas:\")\n",
        "for idx in [0, 5, 10]:  # Puedes ajustar estos índices según necesites\n",
        "    if idx < len(val_dataset):\n",
        "        print(f\"Imagen {idx}:\")\n",
        "        visualize_comparison(idx)"
      ],
      "metadata": {
        "id": "oxBGvIK9U62U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "r2Pk1UgQXaO-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}