{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BL-oYKcKICDz"
      },
      "source": [
        "# 2. Finetuning YOLOv5: ``You-Only-Look-Once\""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Actividad 4\n",
        "Investigue sobre la arquitectura de los modelos YOLO y explique los siguientes conceptos (para cada\n",
        "caso use solo 5 líneas máximo):\n",
        "\n",
        "\n",
        "• Diferencia entre frameworks de detección de objetos de una y dos etapas (single-stage vs.\n",
        "two-stage object detection).\n",
        "\n",
        "\n",
        "• Función de pérdida Complete Intersection Over Union (CIoU) y su uso en la red YOLOv5.\n",
        "\n",
        "\n",
        "• Función de pérdida Binary Cross Entropy (BCE) y su uso en la red YOLOv5.\n",
        "\n",
        "• ¿Cómo se calcula la función de pérdida total en YOLOv5?"
      ],
      "metadata": {
        "id": "yG6_FdclFGxg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "## 1. Single‑stage vs. Two‑stage object detection\n",
        "- **Modelos de una etapa (single‑stage)**, como YOLO, *predicen directamente* bounding boxes + clases en un **único paso** sobre la imagen, obteniendo gran velocidad.\n",
        "- **Modelos de dos etapas (two‑stage)**, como Faster R‑CNN, primero generan **regiones propuestas (RPN)** y luego **refinan y clasifican** cada propuesta.\n",
        "- El enfoque de una etapa sacrifica algo de exactitud por latencia bajísima; el de dos etapas logra mejor mAP pero con mayor coste computacional.\n",
        "- En tareas de *streaming* o tiempo real, la relación *precisión/velocidad* de YOLO lo hace preferible.\n",
        "\n",
        "## 2. Pérdida Complete IoU (CIoU)\n",
        "La métrica CIoU extiende la IoU clásica penalizando también la distancia entre centros y la discrepancia de aspecto:\n",
        "\n",
        "$$\n",
        "\\text{CIoU}(b, b^{gt}) = \\text{IoU}\n",
        "- \\frac{\\rho^2(\\mathbf{c}, \\mathbf{c^{gt}})}{c^2}\n",
        "- \\alpha v\n",
        "$$\n",
        "\n",
        "con $\\rho$ la distancia euclídea entre centros, $c$ la diagonal del cuadro mínimo que los contiene, $v$ la divergencia de relación ancho‑alto y $\\alpha$ un término de equilibrio.\n",
        "\n",
        "- CIoU es **diferenciable** y conduce a convergencia más estable que IoU o GIoU.\n",
        "- YOLOv5 optimiza la posición de los *boxes* usando $L_\\text{CIoU} = 1 - \\text{CIoU}$.\n",
        "\n",
        "## 3. Pérdida Binary Cross Entropy (BCE)\n",
        "Para un ejemplo con etiqueta binaria $y \\in \\{0,1\\}$ y probabilidad predicha $p$:\n",
        "\n",
        "$$\n",
        "\\text{BCE}(p, y) = -\\big(y \\log p + (1 - y) \\log(1 - p)\\big)\n",
        "$$\n",
        "\n",
        "- En YOLOv5 hay **dos instancias** de BCE:\n",
        "  1. **Objectness**: decide si existe objeto en cada celda‑ancla.\n",
        "  2. **Clasificación**: distribuye probabilidad entre las $N$ clases (BCE por clase, activación sigmoid).\n",
        "- BCE es adecuada para etiquetas escasas y balances asimétricos.\n",
        "\n",
        "## 4. Función de pérdida total en YOLOv5\n",
        "La red minimiza una suma ponderada de tres componentes:\n",
        "\n",
        "$$\n",
        "\\mathcal{L}_\\text{total}\n",
        "= \\lambda_{\\text{box}}\\,\\mathcal{L}_{\\text{CIoU}}\n",
        "+ \\lambda_{\\text{obj}}\\,\\mathcal{L}_{\\text{BCE,obj}}\n",
        "+ \\lambda_{\\text{cls}}\\,\\mathcal{L}_{\\text{BCE,cls}}\n",
        "$$\n",
        "\n",
        "- $\\mathcal{L}_{\\text{CIoU}}$ ajusta posición y tamaño de los *bounding boxes*.\n",
        "- $\\mathcal{L}_{\\text{BCE,obj}}$ refuerza la correcta predicción de presencia de objeto.\n",
        "- $\\mathcal{L}_{\\text{BCE,cls}}$ impulsa la clasificación de la clase.\n",
        "- Los pesos $\\lambda$ se afinan empíricamente, típicamente $\\approx (0.05, 1.0, 0.5)$, para equilibrar localización y reconocimiento.\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "A1aJXwJCFVtK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Actividad 5\n",
        "Investigue sobre el output de la red YOLOv5 y cómo se traduce el tensor de salida a bounding boxes\n",
        "y detecciones de objetos. Además, explique cómo, en general, se obtiene solamente una detección\n",
        "por objeto, y no varias para todas las regiones donde el objeto está presente. Además, investigue el\n",
        "rol de la augmentación de datos en el entrenamiento de YOLOv5."
      ],
      "metadata": {
        "id": "Fp33Qw3LJMRX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## 1. Decodificación del tensor de salida a *bounding boxes*\n",
        "YOLOv5 produce un tensor de forma `(B, A*(5+C), H, W)` donde:\n",
        "- *B* = batch, *A* = anclas por celda, *C* = número de clases, *H×W* = tamaño de la malla.\n",
        "- Para cada ancla la red predice $\\big(t_x,\\,t_y,\\,t_w,\\,t_h,\\,p_{obj},\\,p_{cls,1\\dots C}\\big)$.\n",
        "\n",
        "Las coordenadas se decodifican con\n",
        "$$\n",
        "\\begin{aligned}\n",
        " x &= \\frac{\\sigma(t_x)+c_x}{W},\\\\\n",
        " y &= \\frac{\\sigma(t_y)+c_y}{H},\\\\\n",
        " w &= \\frac{e^{t_w}\\,a_w}{\\text{img}_w},\\\\\n",
        " h &= \\frac{e^{t_h}\\,a_h}{\\text{img}_h},\n",
        "\\end{aligned}\n",
        "$$\n",
        "donde $(c_x,c_y)$ son las coordenadas de la celda, $(a_w,a_h)$ las dimensiones del ancla y $\\sigma$ la sigmoide. Así se obtienen *bounding boxes* normalizados $(x,y,w,h)$.\n",
        "\n",
        "## 2. Una sola detección por objeto: *Non‑Max Suppression* (NMS)\n",
        "- Después de decodificar, cada *box* lleva una confianza $p = \\sigma(p_{obj})\\max_j p_{cls,j}$.\n",
        "- Se descartan boxes con $p$ bajo (threshold).\n",
        "- Sobre los restantes se aplica **NMS**: se mantienen los boxes con mayor confianza y se eliminan los que tengan IoU $>\\tau$ con alguno ya aceptado.\n",
        "- YOLOv5 usa variantes como *CIoU‑NMS* o *Weighted NMS* según la versión, pero el principio es idéntico: una sola predicción por objeto.\n",
        "\n",
        "## 3. Rol del aumento de datos (*data augmentation*)\n",
        "El *augment* amplía la diversidad del set y mejora la generalización:\n",
        "- **Geometría**: escalado, recorte, volteo, rotaciones (útiles cuando la orientación del satélite varía).\n",
        "- **Color**: jitter HSV, blur, ruido.\n",
        "- **Mosaic & MixUp**: combinan cuatro imágenes o mezclan pares, exponiendo múltiples objetos a cada contexto.\n",
        "- Beneficios principales: reduce *overfitting*, mejora robustez ante variaciones de iluminación/escala y estabiliza el entrenamiento cuando los datos son escasos.\n",
        "\n",
        "En YOLOv5 el módulo `Albumentations` (o la implementación interna de Ultralytics) integra estos *transforms* antes de pasar las imágenes por la red.\n"
      ],
      "metadata": {
        "id": "ZxFgHXl5FJJS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Actividad 6\n",
        "En el notebook, deberán hacer fine-tuning para que una instancia de YOLOv5 pueda realizar seg-\n",
        "mentaciones de datos satelitales. Para esto, se encuentra implementado la carga del modelo YOLOv5\n",
        "\n",
        "con los parámetros congelados salvo la cabeza de detección. Se encuentra además el código para\n",
        "cargar y preprocesar el set de datos. Se deben completar las siguientes funciones:"
      ],
      "metadata": {
        "id": "HZ7E6eC8Kiby"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TLKMzG0M3fTr",
        "outputId": "12da42e0-c5dd-4a39-9449-3f3a6de90d88"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: roboflow in /usr/local/lib/python3.11/dist-packages (1.1.63)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from roboflow) (2025.4.26)\n",
            "Requirement already satisfied: idna==3.7 in /usr/local/lib/python3.11/dist-packages (from roboflow) (3.7)\n",
            "Requirement already satisfied: cycler in /usr/local/lib/python3.11/dist-packages (from roboflow) (0.12.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from roboflow) (1.4.8)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from roboflow) (3.10.0)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.11/dist-packages (from roboflow) (2.0.2)\n",
            "Requirement already satisfied: opencv-python-headless==4.10.0.84 in /usr/local/lib/python3.11/dist-packages (from roboflow) (4.10.0.84)\n",
            "Requirement already satisfied: Pillow>=7.1.2 in /usr/local/lib/python3.11/dist-packages (from roboflow) (11.2.1)\n",
            "Requirement already satisfied: pillow-heif>=0.18.0 in /usr/local/lib/python3.11/dist-packages (from roboflow) (0.22.0)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.11/dist-packages (from roboflow) (2.9.0.post0)\n",
            "Requirement already satisfied: python-dotenv in /usr/local/lib/python3.11/dist-packages (from roboflow) (1.1.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from roboflow) (2.32.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from roboflow) (1.17.0)\n",
            "Requirement already satisfied: urllib3>=1.26.6 in /usr/local/lib/python3.11/dist-packages (from roboflow) (2.4.0)\n",
            "Requirement already satisfied: tqdm>=4.41.0 in /usr/local/lib/python3.11/dist-packages (from roboflow) (4.67.1)\n",
            "Requirement already satisfied: PyYAML>=5.3.1 in /usr/local/lib/python3.11/dist-packages (from roboflow) (6.0.2)\n",
            "Requirement already satisfied: requests-toolbelt in /usr/local/lib/python3.11/dist-packages (from roboflow) (1.0.0)\n",
            "Requirement already satisfied: filetype in /usr/local/lib/python3.11/dist-packages (from roboflow) (1.2.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->roboflow) (1.3.2)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->roboflow) (4.57.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->roboflow) (24.2)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->roboflow) (3.2.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->roboflow) (3.4.1)\n",
            "loading Roboflow workspace...\n",
            "loading Roboflow project...\n"
          ]
        }
      ],
      "source": [
        "!pip install roboflow\n",
        "# Para cargar un dataset desde Roboflow\n",
        "from roboflow import Roboflow\n",
        "my_key = \"hhJFVyqL27We1hzvzNAa\"\n",
        "rf = Roboflow(api_key=my_key)\n",
        "project = rf.workspace(\"cvproject-y6bf4\").project(\"vehicle-detection-gr77r\")\n",
        "version = project.version(1)\n",
        "dataset = version.download(\"yolov5\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ywGGeJWGhCmU",
        "outputId": "12b3f887-a6d6-4f20-ac3f-adfde72ece90"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'yolov5' already exists and is not an empty directory.\n",
            "/content/yolov5\n",
            "Requirement already satisfied: gitpython>=3.1.30 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 5)) (3.1.44)\n",
            "Requirement already satisfied: matplotlib>=3.3 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 6)) (3.10.0)\n",
            "Requirement already satisfied: numpy>=1.23.5 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 7)) (2.0.2)\n",
            "Requirement already satisfied: opencv-python>=4.1.1 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 8)) (4.11.0.86)\n",
            "Requirement already satisfied: pillow>=10.3.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 9)) (11.2.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 10)) (5.9.5)\n",
            "Requirement already satisfied: PyYAML>=5.3.1 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 11)) (6.0.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 12)) (2.32.3)\n",
            "Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 13)) (1.15.2)\n",
            "Requirement already satisfied: thop>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 14)) (0.1.1.post2209072238)\n",
            "Requirement already satisfied: torch>=1.8.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 15)) (2.6.0+cu124)\n",
            "Requirement already satisfied: torchvision>=0.9.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 16)) (0.21.0+cu124)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 17)) (4.67.1)\n",
            "Requirement already satisfied: ultralytics>=8.2.34 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 18)) (8.3.128)\n",
            "Requirement already satisfied: pandas>=1.1.4 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 27)) (2.2.2)\n",
            "Requirement already satisfied: seaborn>=0.11.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 28)) (0.13.2)\n",
            "Requirement already satisfied: setuptools>=70.0.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 42)) (75.2.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython>=3.1.30->-r requirements.txt (line 5)) (4.0.12)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3->-r requirements.txt (line 6)) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3->-r requirements.txt (line 6)) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3->-r requirements.txt (line 6)) (4.57.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3->-r requirements.txt (line 6)) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3->-r requirements.txt (line 6)) (24.2)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3->-r requirements.txt (line 6)) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3->-r requirements.txt (line 6)) (2.9.0.post0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->-r requirements.txt (line 12)) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->-r requirements.txt (line 12)) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->-r requirements.txt (line 12)) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->-r requirements.txt (line 12)) (2025.4.26)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->-r requirements.txt (line 15)) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->-r requirements.txt (line 15)) (4.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->-r requirements.txt (line 15)) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->-r requirements.txt (line 15)) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->-r requirements.txt (line 15)) (2025.3.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->-r requirements.txt (line 15)) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->-r requirements.txt (line 15)) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->-r requirements.txt (line 15)) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->-r requirements.txt (line 15)) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->-r requirements.txt (line 15)) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->-r requirements.txt (line 15)) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->-r requirements.txt (line 15)) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->-r requirements.txt (line 15)) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->-r requirements.txt (line 15)) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->-r requirements.txt (line 15)) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->-r requirements.txt (line 15)) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->-r requirements.txt (line 15)) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->-r requirements.txt (line 15)) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->-r requirements.txt (line 15)) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->-r requirements.txt (line 15)) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.8.0->-r requirements.txt (line 15)) (1.3.0)\n",
            "Requirement already satisfied: py-cpuinfo in /usr/local/lib/python3.11/dist-packages (from ultralytics>=8.2.34->-r requirements.txt (line 18)) (9.0.0)\n",
            "Requirement already satisfied: ultralytics-thop>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics>=8.2.34->-r requirements.txt (line 18)) (2.0.14)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.1.4->-r requirements.txt (line 27)) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.1.4->-r requirements.txt (line 27)) (2025.2)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython>=3.1.30->-r requirements.txt (line 5)) (5.0.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib>=3.3->-r requirements.txt (line 6)) (1.17.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.8.0->-r requirements.txt (line 15)) (3.0.2)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.11/dist-packages (4.11.0.86)\n",
            "Requirement already satisfied: numpy>=1.21.2 in /usr/local/lib/python3.11/dist-packages (from opencv-python) (2.0.2)\n",
            "Requirement already satisfied: torchinfo in /usr/local/lib/python3.11/dist-packages (1.8.0)\n",
            "--2025-05-07 02:39:24--  https://github.com/ultralytics/yolov5/releases/download/v7.0/yolov5s.pt\n",
            "Resolving github.com (github.com)... 140.82.113.4\n",
            "Connecting to github.com (github.com)|140.82.113.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://objects.githubusercontent.com/github-production-release-asset-2e65be/264818686/381bd8a8-8910-4e9e-b0dd-2752951ef78c?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=releaseassetproduction%2F20250507%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20250507T023924Z&X-Amz-Expires=300&X-Amz-Signature=45426fcbb86b5a4a93028a509e2374f5d207c8a452c9ed97d705ca937473986d&X-Amz-SignedHeaders=host&response-content-disposition=attachment%3B%20filename%3Dyolov5s.pt&response-content-type=application%2Foctet-stream [following]\n",
            "--2025-05-07 02:39:24--  https://objects.githubusercontent.com/github-production-release-asset-2e65be/264818686/381bd8a8-8910-4e9e-b0dd-2752951ef78c?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=releaseassetproduction%2F20250507%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20250507T023924Z&X-Amz-Expires=300&X-Amz-Signature=45426fcbb86b5a4a93028a509e2374f5d207c8a452c9ed97d705ca937473986d&X-Amz-SignedHeaders=host&response-content-disposition=attachment%3B%20filename%3Dyolov5s.pt&response-content-type=application%2Foctet-stream\n",
            "Resolving objects.githubusercontent.com (objects.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to objects.githubusercontent.com (objects.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 14808437 (14M) [application/octet-stream]\n",
            "Saving to: ‘yolov5s.pt.2’\n",
            "\n",
            "yolov5s.pt.2        100%[===================>]  14.12M  --.-KB/s    in 0.1s    \n",
            "\n",
            "2025-05-07 02:39:24 (108 MB/s) - ‘yolov5s.pt.2’ saved [14808437/14808437]\n",
            "\n",
            "/content\n"
          ]
        }
      ],
      "source": [
        "# Instalar dependencias de YOLOv5\n",
        "!git clone https://github.com/ultralytics/yolov5\n",
        "%cd yolov5\n",
        "!pip install -r requirements.txt\n",
        "!pip install opencv-python\n",
        "!pip install torchinfo\n",
        "!wget https://github.com/ultralytics/yolov5/releases/download/v7.0/yolov5s.pt\n",
        "%cd .."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "XVcxOFJp3mkw"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "sys.path.append('yolov5')\n",
        "import os\n",
        "import cv2\n",
        "import torch\n",
        "import numpy as np\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from PIL import Image\n",
        "from torchvision import transforms as T\n",
        "from models.yolo import Model, Detect\n",
        "import yaml\n",
        "from utils.loss import ComputeLoss\n",
        "from torchinfo import summary\n",
        "import albumentations as A"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "collapsed": true,
        "id": "OWd07IAW4ZJ9"
      },
      "outputs": [],
      "source": [
        "# Dataset de YOLO\n",
        "\n",
        "class YoloDataset(Dataset):\n",
        "    def __init__(self, root, img_size=640, augment=False):\n",
        "        self.image_dir = os.path.join(root, 'images')\n",
        "        self.label_dir = os.path.join(root, 'labels')\n",
        "        self.filenames = sorted(os.listdir(self.image_dir))[:2]\n",
        "        self.img_size = img_size\n",
        "\n",
        "        transforms =[\n",
        "              A.Resize(img_size, img_size),\n",
        "              A.Normalize(mean=(0.0, 0.0, 0.0), std=(1.0, 1.0, 1.0)),\n",
        "              A.ToTensorV2()\n",
        "              ]\n",
        "        if augment:\n",
        "            # Completar con rotaciones\n",
        "          pass\n",
        "\n",
        "        self.transform = A.Compose(transforms)\n",
        "        self.yaml_path = '/' + os.path.join(*root.split('/')[:-1], 'data.yaml')\n",
        "        with open(self.yaml_path, 'r') as f:\n",
        "            self.yaml = yaml.safe_load(f)\n",
        "        self.cls_to_idx = {cls: idx for idx, cls in enumerate(self.yaml['names'])}\n",
        "        self.idx_to_cls = {idx: cls for idx, cls in enumerate(self.yaml['names'])}\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_name = self.filenames[idx]\n",
        "        img_path = os.path.join(self.image_dir, img_name)\n",
        "        label_path = os.path.join(self.label_dir, img_name.replace('.jpg', '.txt').replace('.png', '.txt'))\n",
        "        img = np.array(Image.open(img_path).convert('RGB'))\n",
        "        labels = []\n",
        "        classes = []\n",
        "        if os.path.exists(label_path):\n",
        "            with open(label_path, 'r') as f:\n",
        "                for line in f:\n",
        "                    vals = list(map(float, line.strip().split()))\n",
        "                    if len(vals) >= 5:\n",
        "                        cls, bbox = int(vals[0]), vals[1:5]\n",
        "                        classes.append(cls)\n",
        "                        labels.append(bbox)\n",
        "        transformed = self.transform(image=img, bboxes=labels, class_labels=classes)\n",
        "        img = transformed['image']\n",
        "        boxes = torch.tensor(transformed['bboxes'], dtype=torch.float32)\n",
        "        classes = torch.tensor(transformed['class_labels'], dtype=torch.float32).unsqueeze(1)\n",
        "        if boxes.numel() > 0:\n",
        "            labels = torch.cat([classes, boxes], dim=1)\n",
        "        else:\n",
        "            labels = torch.zeros((0, 5), dtype=torch.float32)\n",
        "\n",
        "        return img, labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.filenames)\n",
        "\n",
        "# Convierte datos a formato YOLO\n",
        "def collate_fn(batch):\n",
        "    imgs, targets = [], []\n",
        "    for i, (img, target) in enumerate(batch):\n",
        "        imgs.append(img)\n",
        "        target = torch.cat([torch.full((target.size(0), 1), i), target], dim=1)\n",
        "        targets.append(target)\n",
        "\n",
        "    imgs = torch.stack(imgs, dim=0)\n",
        "    targets = torch.cat(targets, dim=0)\n",
        "\n",
        "    return imgs, targets\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "-yafJLA_WA7A",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "749b339f-109d-4020-e200-f8808d447288"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using cache found in /root/.cache/torch/hub/ultralytics_yolov5_master\n",
            "YOLOv5 🚀 v7.0-416-gfe1d4d99 Python-3.11.12 torch-2.6.0+cu124 CPU\n",
            "\n",
            "Overriding model.yaml nc=80 with nc=4\n",
            "\n",
            "                 from  n    params  module                                  arguments                     \n",
            "  0                -1  1      3520  models.common.Conv                      [3, 32, 6, 2, 2]              \n",
            "  1                -1  1     18560  models.common.Conv                      [32, 64, 3, 2]                \n",
            "  2                -1  1     18816  models.common.C3                        [64, 64, 1]                   \n",
            "  3                -1  1     73984  models.common.Conv                      [64, 128, 3, 2]               \n",
            "  4                -1  2    115712  models.common.C3                        [128, 128, 2]                 \n",
            "  5                -1  1    295424  models.common.Conv                      [128, 256, 3, 2]              \n",
            "  6                -1  3    625152  models.common.C3                        [256, 256, 3]                 \n",
            "  7                -1  1   1180672  models.common.Conv                      [256, 512, 3, 2]              \n",
            "  8                -1  1   1182720  models.common.C3                        [512, 512, 1]                 \n",
            "  9                -1  1    656896  models.common.SPPF                      [512, 512, 5]                 \n",
            " 10                -1  1    131584  models.common.Conv                      [512, 256, 1, 1]              \n",
            " 11                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
            " 12           [-1, 6]  1         0  models.common.Concat                    [1]                           \n",
            " 13                -1  1    361984  models.common.C3                        [512, 256, 1, False]          \n",
            " 14                -1  1     33024  models.common.Conv                      [256, 128, 1, 1]              \n",
            " 15                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
            " 16           [-1, 4]  1         0  models.common.Concat                    [1]                           \n",
            " 17                -1  1     90880  models.common.C3                        [256, 128, 1, False]          \n",
            " 18                -1  1    147712  models.common.Conv                      [128, 128, 3, 2]              \n",
            " 19          [-1, 14]  1         0  models.common.Concat                    [1]                           \n",
            " 20                -1  1    296448  models.common.C3                        [256, 256, 1, False]          \n",
            " 21                -1  1    590336  models.common.Conv                      [256, 256, 3, 2]              \n",
            " 22          [-1, 10]  1         0  models.common.Concat                    [1]                           \n",
            " 23                -1  1   1182720  models.common.C3                        [512, 512, 1, False]          \n",
            " 24      [17, 20, 23]  1     24273  models.yolo.Detect                      [4, [[10, 13, 16, 30, 33, 23], [30, 61, 62, 45, 59, 119], [116, 90, 156, 198, 373, 326]], [128, 256, 512]]\n",
            "YOLOv5s summary: 214 layers, 7030417 parameters, 7030417 gradients, 16.0 GFLOPs\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import yaml\n",
        "from models.yolo import Model\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "train_dataset = YoloDataset('/content/Vehicle-Detection-1/train')\n",
        "\n",
        "train_loader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=1,           # antes era 128\n",
        "    shuffle=True,\n",
        "    collate_fn=collate_fn,\n",
        "    num_workers=0            # evita problemas paralelos en RAM/Colab\n",
        ")\n",
        "\n",
        "\n",
        "model = torch.hub.load(\"ultralytics/yolov5\", \"yolov5s\", classes=len(train_dataset.yaml['names']), pretrained=True, autoshape=False).cpu()\n",
        "hyp_path = '/content/yolov5/data/hyps/hyp.scratch-low.yaml'\n",
        "\n",
        "with open(hyp_path) as f:\n",
        "    hyp = yaml.safe_load(f)\n",
        "\n",
        "# Congelar modelo\n",
        "for param in model.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "# Descongelar cabeza\n",
        "for param in model.model[-1].parameters():\n",
        "    param.requires_grad = True\n",
        "\n",
        "model = model.to('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model.hyp = hyp\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "DQwyDaPjVT2O"
      },
      "outputs": [],
      "source": [
        "from yolov5.utils.augmentations import Albumentations\n",
        "from yolov5.utils.loss import ComputeLoss\n",
        "\n",
        "def train(model, dataloader, optimizer, epochs, transforms):\n",
        "    \"\"\"\n",
        "    Finetunea la cabeza de detección de YOLOv5.\n",
        "    Usa Albumentations(transforms) y ComputeLoss (CIoU + BCE obj/cls).\n",
        "    Imprime el promedio de cada término al final de cada epoch.\n",
        "    \"\"\"\n",
        "    model.model.transform = transforms\n",
        "    compute_loss = ComputeLoss(model)\n",
        "    device = next(model.parameters()).device\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        tot_l, box_l, obj_l, cls_l = 0.0, 0.0, 0.0, 0.0\n",
        "\n",
        "        for imgs, targets in dataloader:\n",
        "            imgs, targets = imgs.to(device), targets.to(device)\n",
        "\n",
        "            preds = model(imgs)\n",
        "            loss, (l_box, l_obj, l_cls) = compute_loss(preds, targets)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            tot_l  += loss.item()\n",
        "            box_l  += l_box.item()\n",
        "            obj_l  += l_obj.item()\n",
        "            cls_l  += l_cls.item()\n",
        "\n",
        "        n = len(dataloader)\n",
        "        print(f\"Epoch {epoch+1}/{epochs} | total={tot_l/n:.4f} \"\n",
        "              f\"| box={box_l/n:.4f} | obj={obj_l/n:.4f} | cls={cls_l/n:.4f}\")\n",
        "\n",
        "\n",
        "\n",
        "from yolov5.utils.loss import ComputeLoss\n",
        "\n",
        "def eval(model, dataloader):\n",
        "    \"\"\"\n",
        "    Calcula los promedios de CIoU, BCE‑class y BCE‑object sin retro‑propagación.\n",
        "    Imprime los resultados al final.\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    compute_loss = ComputeLoss(model)\n",
        "    device = next(model.parameters()).device\n",
        "\n",
        "    ciou_sum, bce_obj_sum, bce_cls_sum, n_batches = 0.0, 0.0, 0.0, 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for imgs, targets in dataloader:\n",
        "            imgs, targets = imgs.to(device), targets.to(device)\n",
        "\n",
        "            preds = model(imgs)\n",
        "            _, (l_box, l_obj, l_cls) = compute_loss(preds, targets)\n",
        "\n",
        "            ciou_sum   += l_box.item()\n",
        "            bce_obj_sum += l_obj.item()\n",
        "            bce_cls_sum += l_cls.item()\n",
        "            n_batches  += 1\n",
        "\n",
        "    print(f\"Mean scores: CIoU {ciou_sum/n_batches:.4f}, \"\n",
        "          f\"BCE class: {bce_cls_sum/n_batches:.4f}, \"\n",
        "          f\"BCE object: {bce_obj_sum/n_batches:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "eval(model, train_loader)   # o val_loader si lo tienes\n",
        "\n"
      ],
      "metadata": {
        "id": "_7TfAplOkti0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 304
        },
        "outputId": "532025fa-6ff3-4e81-b7cd-cf5effd55c84"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "index 3 is out of bounds for dimension 0 with size 3",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-564f425b03b7>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m   \u001b[0;31m# o val_loader si lo tienes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-17-910806fe30cc>\u001b[0m in \u001b[0;36meval\u001b[0;34m(model, dataloader)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m             \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimgs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m             \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0ml_box\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml_obj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml_cls\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m             \u001b[0mciou_sum\u001b[0m   \u001b[0;34m+=\u001b[0m \u001b[0ml_box\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/yolov5/utils/loss.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, p, targets)\u001b[0m\n\u001b[1;32m    142\u001b[0m         \u001b[0mlbox\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# box loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m         \u001b[0mlobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# object loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 144\u001b[0;31m         \u001b[0mtcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtbox\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0manchors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild_targets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# targets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    145\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m         \u001b[0;31m# Losses\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/yolov5/utils/loss.py\u001b[0m in \u001b[0;36mbuild_targets\u001b[0;34m(self, p, targets)\u001b[0m\n\u001b[1;32m    217\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m             \u001b[0manchors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0manchors\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 219\u001b[0;31m             \u001b[0mgain\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# xyxy gain\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    220\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m             \u001b[0;31m# Match targets to anchors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: index 3 is out of bounds for dimension 0 with size 3"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "F4xzinlYo_wx"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}